{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c23411ad-4bdf-4f97-9b3e-0df42a06c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import imageio\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from scipy.optimize import curve_fit\n",
    "from joblib import Parallel, delayed\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "\n",
    "# get all the colors\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39babdcd-ad96-450b-abe0-41190ca85bf0",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2744a9bb-1130-4b0f-a9d2-8f85c0b3dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxgr\\.matplotlib\n"
     ]
    }
   ],
   "source": [
    "boundary_conditions = {0 : \"PBC\", 1:\"OBC\"}\n",
    "print(matplotlib.get_configdir())\n",
    "#plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86c3e1ad-c589-4928-a43e-d651b50741b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingDisorder:\n",
    "    N = 1\n",
    "    M = 1\n",
    "    def __init__(self, L, J, J0, g, g0, h, w, _BC):\n",
    "        self.L = L\n",
    "        self.J = J\n",
    "        self.J0 = J0\n",
    "        self.g=g\n",
    "        self.g0=g0\n",
    "        self.h=h\n",
    "        self.w =w\n",
    "        self.BC=_BC\n",
    "        self.directory = \"results\" + kPSep\n",
    "        self.N = math.pow(2,L)\n",
    "        \n",
    "    def getInfo(self):\n",
    "        return f'L={self.L},J0={self.J0:.2f},g={self.g:.2f},g0={self.g0:.2f},h={self.h:.2f},w={self.w:.2f}' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb2dfa0a-8338-4b90-b662-c749f3a82a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kPSep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8824/3565150683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIsingDisorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8824/467981809.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, L, J, J0, g, g0, h, w, _BC)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_BC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"results\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kPSep' is not defined"
     ]
    }
   ],
   "source": [
    "a=IsingDisorder(12,1,0,0.8,0,0.8,0.1,0)\n",
    "a.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c3491-b676-411a-9a5f-492d0eb23c22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b92c6b91-6c88-4724-9267-56391f765f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Uni\\\\SEMESTERS\\\\PRACE\\\\CONDENSED_GROUP_CLOUD_UNI\\\\Transverse_Ising\\\\Transverse_Ising_ETH\\\\IsignTransverse_ETH'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers = itertools.cycle(['o','s','v', '+'])\n",
    "colors_ls = list(mcolors.TABLEAU_COLORS)[:30]\n",
    "colors = itertools.cycle(sns.color_palette()[:3])\n",
    "TWOPI = math.pi * 2\n",
    "kPSep = os.sep\n",
    "\n",
    "disorder_pbc = f\"..{kPSep}results{kPSep}disorder{kPSep}PBC{kPSep}\"\n",
    "symm_pbc = f\"C:{kPSep}Users{kPSep}maxgr{kPSep}Desktop{kPSep}Wyniken{kPSep}results2_2{kPSep}symmetries{kPSep}PBC{kPSep}\"\n",
    "IsingPath = f\"D:{kPSep}Uni{kPSep}SEMESTERS{kPSep}PRACE{kPSep}CONDENSED_GROUP_CLOUD_UNI{kPSep}Transverse_Ising{kPSep}Transverse_Ising_ETH{kPSep}\"+\\\n",
    "    f\"IsignTransverse_ETH\"\n",
    "IsingPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c4bdb78-e53e-47fc-b7ca-757b769b5e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.001 , 0.0019, 0.0028, 0.0037, 0.0046, 0.0055, 0.0064, 0.0073,\n",
       "        0.0082, 0.0091, 0.01  , 0.01  , 0.03  , 0.05  , 0.07  , 0.09  ,\n",
       "        0.11  , 0.13  , 0.15  , 0.17  , 0.19  , 0.21  , 0.23  , 0.25  ,\n",
       "        0.27  , 0.29  , 0.31  , 0.33  , 0.35  , 0.37  , 0.39  ]),\n",
       " [1.15, 1.17, 1.19])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=1.15\n",
    "hstep = 0.02\n",
    "\n",
    "hvalues = [h+i*hstep for i in range(3)]#+[1.4,1.42,1.45,1.47,1.5,1.55,1.57]\n",
    "# perturbations\n",
    "pert_vec = np.linspace(1e-3, 1e-2, 11);\n",
    "pert_vec = np.append(pert_vec,np.linspace(1e-2, 3.9e-1, 20));\n",
    "pert_vec,hvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720a31a-ef2a-4ab7-8fe4-063f9895afa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------\n",
    "# String separators and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7a74b-8340-450c-b36d-5e3b0d5f8cb5",
   "metadata": {},
   "source": [
    "### Concatenate list to a string given a separator\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13f0cab0-af8c-44ea-94f6-18d0c85a69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert  \n",
    "def listToString(s, separator = \",\"): \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"   \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        if isinstance(ele, (float,int)):\n",
    "            str1 += \"{:.3f}\".format(ele) +separator \n",
    "        else:\n",
    "            str1 += str(ele) +separator  \n",
    "    \n",
    "    # return string  \n",
    "    return str1[:-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf76e4f-541b-46ed-a946-c82876350bb5",
   "metadata": {},
   "source": [
    "### Creating a folder given a directory\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "580bc86e-7e1f-44cd-b1cf-ba3458769ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(directories, silent = False):\n",
    "    for folder in directories:\n",
    "        try:\n",
    "            if not os.path.isdir(folder):\n",
    "                os.makedirs(folder)\n",
    "                if not silent:\n",
    "                    print(\"Created a directory : \", folder)\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" % folder)      \n",
    "# Guard against race condition\n",
    "        except OSError as exc: \n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3d2a2-7b23-4dfb-9e3c-a4e1d5303909",
   "metadata": {},
   "source": [
    "### Reading random number from a folder given a condition\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20937d58-6772-48ca-b22e-948426d795c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = lambda x : x.endswith('.dat')\n",
    "def readRandomFile(folder, cond, withoutFolder = False):\n",
    "    choice = random.choice(os.listdir(folder))\n",
    "    #print(choice)\n",
    "    maxlen = len(os.listdir(folder))\n",
    "    counter = 0\n",
    "    while not cond(choice):\n",
    "        choice = random.choice(os.listdir(folder))\n",
    "        if counter > maxlen:\n",
    "            raise\n",
    "        counter += 1\n",
    "    if withoutFolder:\n",
    "        return choice\n",
    "    else:\n",
    "        return folder + choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d1e99-bbdb-4c6d-98a2-0c01a3ad5b24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### finding things in the list that are the same not to print them twice\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6990cf6f-49cc-4c45-9ec7-71af692d004e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('g=1,pert=0.2', ['w=3,h=2,a=1', 'h=3,w=4,a=1', 'w=4,h=2,a=2'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findDifferentElementsStrings(listOfStrings):\n",
    "    different = []\n",
    "    the_same = []\n",
    "    if len(listOfStrings) != 0:\n",
    "        # create the intersection to distinguish same elements\n",
    "        same = set(listOfStrings[0].split(\",\"))\n",
    "        #print(same)\n",
    "        for i in range(1,len(listOfStrings)):\n",
    "            tmp = set(listOfStrings[i].split(\",\"))\n",
    "            same = same.intersection(tmp)\n",
    "            #print(same)\n",
    "        # after having the intersection find the list of names to put into legend that are different\n",
    "        for i in range(0, len(listOfStrings)):\n",
    "            tmp = set(listOfStrings[i].split(\",\")) - same\n",
    "            different.append(listToString(tmp))\n",
    "        the_same = list(same)\n",
    "    # return same and list of different strings in the form of tuple\n",
    "    return (listToString(np.sort(the_same)), different)\n",
    "\n",
    "findDifferentElementsStrings([\"g=1,h=2,w=3,a=1,pert=0.2\", \"g=1,h=3,w=4,a=1,pert=0.2\",\"g=1,h=2,w=4,a=2,pert=0.2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b894d-1935-429e-8289-07afd9217efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print to file with adjusting column width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0e4c9ca-a34c-4186-9e2a-d90044b8f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def justPrinter(file,sep=\"\\t\", elements =[], width = 8, endline = True):\n",
    "    for item in elements:\n",
    "        file.write((str(item) + sep).ljust(width))\n",
    "    if endline:\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570338d-fe98-442d-b58e-b60531c9016c",
   "metadata": {},
   "source": [
    "#### return list of axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00a582ec-be77-4cea-8ac8-64b49bea60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfAxis(num, figsize = (10,10), dpi = 100):\n",
    "    fig, ax = plt.subplots(num, figsize=(10,10), dpi = 100)\n",
    "    sns.set_style(\"ticks\")\n",
    "    # set axis for it to always be a list\n",
    "    axis = []\n",
    "    if num > 1: # if we have many columns to plot\n",
    "        axis = [ax[i] for i in range(num)]\n",
    "    else:\n",
    "        axis = [ax]\n",
    "    return fig, axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913347f2-2a41-4299-a344-80cc9949d5bc",
   "metadata": {},
   "source": [
    "------------\n",
    "## IMAGES AND ETC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f6f03-acf4-423c-a98d-43fb41b77f94",
   "metadata": {},
   "source": [
    "#### make gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f18b377-3e3a-4511-906b-fc69f05eb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makegif(directory, name, distinguishers, images,fps=1.0):\n",
    "    folder = f'{directory}plots{kPSep}' \n",
    "    kwargs_write = {'fps':fps, 'quantizer':'nq'}\n",
    "    imageio.mimsave(folder + f'{name}_{listToString(distinguishers)}.gif', images, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f1a6c-83dc-49a2-9fe9-cba73bbaeb96",
   "metadata": {},
   "source": [
    "#### make axis style for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8064bd7-9dfb-4937-b278-77dae17a5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPlotElems(ax, xlabel, ylabel, xscale, yscale, legend = True, leg_names = [], font_size = 8.5, xlim =[], ylim=[]):\n",
    "    ax.set(xlabel = xlabel, ylabel = ylabel)\n",
    "    ax.set_yscale(yscale)\n",
    "    ax.set_xscale(xscale)\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(leg_names\n",
    "                , frameon=False\n",
    "                , loc='best'\n",
    "                , fontsize=font_size)\n",
    "    \n",
    "    xmin, xmax = xlim\n",
    "    ymin, ymax = ylim\n",
    "    if xmin != None and xmax != None:\n",
    "        ax.set_xlim([xmin,xmax])\n",
    "    if ymin != None and ymax != None:\n",
    "        ax.set_ylim([ymin, ymax])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bc0e9-c85e-4058-a830-e1d4daab26f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "-------------------------\n",
    "# Handle all the files starting from a given name given by the user and distinguished by the parameters and return the list of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "970419d8-0cdb-4d22-98d7-48b9b02b693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_header(filename):\n",
    "    with open(filename) as f:\n",
    "        first = f.read(1)\n",
    "        return first not in '.-0123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6876c-4a39-4b96-8e1a-f85b1ab40a0a",
   "metadata": {},
   "source": [
    "### Fit functions\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "672a2ffc-a762-4399-88b3-c320210e8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<function __main__.poisson_fit(x, lambd)>: ('poisson',\n",
       "  '$\\\\lambda$*exp(-$\\\\lambda$x):',\n",
       "  '$\\\\lambda$='),\n",
       " <function __main__.laplace_fit(x, lambd, mu)>: ('laplace',\n",
       "  'exp(-|x-$\\\\mu$|/$\\\\lambda$)/2$\\\\lambda$:',\n",
       "  '$\\\\mu$=',\n",
       "  '$\\\\lambda$='),\n",
       " <function __main__.exponential_fit(x, lambd, sigma)>: ('exponential',\n",
       "  '$\\\\sigma$*exp(-$\\\\lambda$x):',\n",
       "  '$\\\\sigma$=',\n",
       "  '$\\\\lambda$='),\n",
       " <function __main__.gauss_fit(x, mu, sigma)>: ('gaussian',\n",
       "  'exp(-$(x-\\\\mu)^2$/2($\\\\sigma^2$))/($\\\\sigma$*$\\\\sqrt{2\\\\pi}$):',\n",
       "  '$\\\\mu$=',\n",
       "  '$\\\\sigma$='),\n",
       " <function __main__.linear_fit(x, a, b)>: ('linear', 'a*x+b:', 'a=', 'b=')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def poisson_fit(x, lambd):\n",
    "    return lambd * np.exp(-lambd * np.abs(x))\n",
    "    \n",
    "def laplace_fit(x, lambd,mu):\n",
    "    return np.exp(-np.abs(x - mu)/lambd)/(2*lambd)\n",
    "\n",
    "def exponential_fit(x, lambd,sigma):\n",
    "    return sigma * np.exp(-lambd * np.abs(x))\n",
    "\n",
    "def gauss_fit(x, mu, sigma):\n",
    "    return np.exp(-0.5*np.power(x-mu,2)/(sigma*sigma))/(sigma*math.sqrt(2*np.pi))\n",
    "\n",
    "def linear_fit(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "fitDic = {\n",
    "    poisson_fit : ('poisson','$\\lambda$*exp(-$\\lambda$x):' ,'$\\lambda$='),\n",
    "    laplace_fit : ('laplace','exp(-|x-$\\mu$|/$\\lambda$)/2$\\lambda$:',\"$\\mu$=\",'$\\lambda$='),\n",
    "    exponential_fit : ('exponential','$\\sigma$*exp(-$\\lambda$x):', '$\\sigma$=', '$\\lambda$=' ),\n",
    "    gauss_fit : ('gaussian','exp(-$(x-\\mu)^2$/2($\\sigma^2$))/($\\sigma$*$\\sqrt{2\\pi}$):', '$\\mu$=', '$\\sigma$=' ),\n",
    "    linear_fit : ('linear', 'a*x+b:', 'a=', 'b=')\n",
    "}\n",
    "fitDic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350cfd4-3184-42e1-92df-fc43538e6632",
   "metadata": {},
   "source": [
    "##### distinguish between different fits and append it to the legend list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1569a104-f73e-4105-94e8-cc037ad81c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFitFunction(fitFun, appendList, popt, chi_p_value):\n",
    "    tmp = fitDic[fitFun]\n",
    "    fitname = tmp[0] + ',' + tmp[1]\n",
    "    counter = 0\n",
    "    for param in tmp[2:]:\n",
    "        appendList.append(f'{param}' + \"{:.3f}\".format(popt[counter]))\n",
    "        counter += 1\n",
    "    appendList.append(f'\\nChi={chi_p_value:.3f}')\n",
    "    return fitname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74f51a-9d98-41da-bba1-2e6911380e66",
   "metadata": {},
   "source": [
    "##### fit to dataframe from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04474c6d-2a68-4f27-8206-20c7214a7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitToDataframe(fitFun, x, y):\n",
    "    # fit curve\n",
    "    xdata = np.array(x)\n",
    "    ydata = np.array(y)\n",
    "    popt, pcov = curve_fit(fitFun, x, ydata)\n",
    "    predict = np.array(fitFun(xdata, *popt))\n",
    "    \n",
    "    ydataChi = ydata / np.sum(ydata)\n",
    "    popt2, pcov2 = curve_fit(fitFun, x, ydataChi)\n",
    "    predictChi = np.array(fitFun(xdata, *popt2))\n",
    "    predictChi = predictChi / np.sum(predictChi)\n",
    "    \n",
    "    #print(np.sum(ydataChi),np.sum(predictChi))\n",
    "    chi, pval_chi = stats.chisquare(f_obs=ydataChi, f_exp=predictChi)\n",
    "    return popt, chi, pd.DataFrame(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe69532-ee46-4ea8-9358-733c3d524d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------\n",
    "### Give dataframes from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "023a74c5-420d-46ad-8b57-8abaa808dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_all_files(directory, file_begin, columns, distinguishers, separator = \"\\t\\t\", fitfunctions = []):\n",
    "    # columns is a list of tuples of column number and its name [(0:...)] etc -> 0 is an idx\n",
    "    dfs = []\n",
    "    # make fit before\n",
    "       \n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename[-4:] != \".dat\" or not filename.startswith(file_begin):\n",
    "            continue\n",
    "        # split the name and the parameters\n",
    "        splitter = filename[:-4].split(\"_\")\n",
    "        # split each of the parameters \n",
    "        params = splitter[-1].split(\",\")\n",
    "        skip = False\n",
    "        \n",
    "        for element in distinguishers:\n",
    "            in_bucket = False\n",
    "            # check elements in each element_bucket\n",
    "            for bucket in element:\n",
    "                if bucket in params:\n",
    "                    in_bucket = True\n",
    "            # if none of possible parameters from individual bucket is in params => skip\n",
    "            if not in_bucket:\n",
    "                skip = True\n",
    "                break\n",
    "                \n",
    "        if not skip or len(distinguishers) == 0:\n",
    "            skip_rows = 0\n",
    "            # check if there is a header present already\n",
    "            if check_header(directory + filename):\n",
    "                skip_rows = 1\n",
    "            tmp = pd.read_csv(directory + filename, sep = separator, header=None, skiprows = skip_rows, index_col = 0)\n",
    "\n",
    "            # check if nan's are in and skip that column if all are nans\n",
    "            tmp = tmp.loc[:,tmp.notna().all(axis=0)]\n",
    "            # if there is still something to read then read!\n",
    "            if len(columns) > 0:\n",
    "                names = [column[0] for column in columns]\n",
    "                numbers = [column[1] for column in columns]\n",
    "                \n",
    "                # take everything except the index\n",
    "                tmp = tmp[numbers[1:]]\n",
    "                tmp.columns = names[1:]\n",
    "            # append new df to a list of dataframes with its params   \n",
    "            dfs.append((tmp, listToString(params)))\n",
    "            for fitFun in fitfunctions:\n",
    "                # always take the first column for the fit\n",
    "                popt,chi, fit_df = fitToDataframe(fitFun, tmp.index, tmp[columns[1][0]])\n",
    "                fit_df.index = tmp.index\n",
    "                # add labels\n",
    "                params_fit = []\n",
    "                fitname = addFitFunction(fitFun, params_fit,popt, chi)\n",
    "                name,function  = fitname.split(',')\n",
    "                params_fit = [function] + params_fit + [name]\n",
    "                #print(params_fit)\n",
    "                # append to dataframes\n",
    "                dfs.append((fit_df, f'{listToString(params_fit,separator=\":\")},{listToString(params,separator=\",\")}'))    \n",
    "    #print(dfs)\n",
    "    return dfs\n",
    "\n",
    "tmp = handle_all_files(symm_pbc, \"perturbationOperatorsDist\", columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)]\\\n",
    "                       , distinguishers = [[\"h=1.35\"],[\"g=0.80\"],['pert=0.0010']], fitfunctions = [gauss_fit])\n",
    "#tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f543ad-4772-4d61-8b0c-6c6341a2a35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42d99a-50d6-46b5-ae5b-2a87adb9f321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21551e63-89fc-482f-8bae-18abc6aff5bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Operators plotter as a function of energies, plots all elements connecting files with parameters present in the distinguisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3598e364-7c54-4a95-928e-c28781deae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_av_operator_dfs(directory, file_begin, columns, distinguishers, operator_name,\\\n",
    "                         scale_x = \"linear\", scale_y = \"linear\",chi_squared = False,\\\n",
    "                         separator = \"\\t\\t\", fitfunctions = [],gif = False,save_files=True,\\\n",
    "                         xmin = -0.05, xmax = 0.05, ymin = 0.01, ymax = 100):\n",
    "    # clean plot    \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    \n",
    "    fitter = []\n",
    "    for fitFun in fitfunctions:\n",
    "        fitter.append(fitDic[fitFun][0])\n",
    "        \n",
    "    # save in folder\n",
    "    folder = f'{directory}plots{kPSep}' \n",
    "    createFolder([folder])\n",
    "    #print(directory)\n",
    "    # find the list of dataframes\n",
    "    a = handle_all_files(directory, file_begin, columns, distinguishers, separator = separator, fitfunctions = fitfunctions)\n",
    "    if len(a) == 0:\n",
    "        return\n",
    "    else:\n",
    "        #unpack\n",
    "        listOfDf = [item[0] for item in a]\n",
    "        labels = ([item[1] for item in a])\n",
    "        \n",
    "    #print(labels)\n",
    "    # create number of figures to be plotted\n",
    "    fig, axis = listOfAxis(len(columns) - 1, figsize=(10,10), dpi = 100)\n",
    "\n",
    "    # for files\n",
    "    same_and_diff = findDifferentElementsStrings(labels)\n",
    "    savefile = file_begin + \"_\" + same_and_diff[0]\n",
    "    \n",
    "    # plot\n",
    "    colorki = [next(colors) for i in range(len(listOfDf))]\n",
    "    markerki = [next(markers) for i in range(len(listOfDf))]\n",
    "    \n",
    "    # iterate over dataframes\n",
    "    df_iter = 0\n",
    "    for df in listOfDf:\n",
    "        itr = 0\n",
    "        for col in df.columns:\n",
    "            df[col].plot(ax = axis[itr]\n",
    "                    , linewidth=0.5\n",
    "                    , marker=markerki[df_iter]\n",
    "                    , markersize=1.8\n",
    "                    , color = colorki[df_iter])\n",
    "            itr+=1\n",
    "        df_iter += 1\n",
    "    #print(\"same_and_diff is:\",np.sort(same_and_diff[1]))\n",
    "    # set the style\n",
    "    for i in range(len(columns) - 1):\n",
    "        \n",
    "        setPlotElems(axis[i], columns[0][0],\n",
    "                     columns[i+1][0], xscale = scale_x, yscale=scale_y,\n",
    "                     legend = len(listOfDf) > 1, \n",
    "                     leg_names = same_and_diff[1],\n",
    "                     font_size = 8.5,\n",
    "                     xlim =[xmin,xmax], ylim=[ymin,ymax])\n",
    "\n",
    "    \n",
    "    # add title\n",
    "    fig.suptitle(operator_name + \" for \" + same_and_diff[0], y=0.91)\n",
    "    # set fitfunctions\n",
    "    # GIF\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    if gif:\n",
    "        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close()\n",
    "    if save_files:\n",
    "        fits = \".png\"\n",
    "        if(len(fitfunctions) > 0):\n",
    "            fits = f',{listToString(fitter,\",\")}{fits}'\n",
    "            #print(fits)\n",
    "        plt.savefig(f'{folder + savefile},scale_y={scale_y},scale_x={scale_x }{fits}')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "            \n",
    "    #\n",
    "    return image\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96f6d72c-2d30-4fc1-bf95-6dede772f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_x                    \n",
    "#plot_av_operator_dfs(symm_pbc, \"SigmaX\",columns = [(\"E/L\",0), (\"<n|$S_x$|n>\",1)], distinguishers = [], operator_name = \"<n|$S_x$|n>\") \n",
    "# sigma_x prob dist\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSigmaX\",columns = [(\"$S^x_{nn}$\", 0), (\"P($S^x_{nn}$)\",1)], distinguishers = [[\"L=18\"],[\"g=0.80\"], [\"h=1.00\",\"h=1.50\"]],\n",
    "#                      operator_name = \"Probability distribution of $S^x_{nn}$\", scale_y = 'log', fitfunctions = []) \n",
    "# sigma x repulsion\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSpecRapSigmaX\",columns = [(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)],\n",
    " #distinguishers = [[\"L=19\",\"L=18\", \"L=17\", \"L=16\"],[\"g=0.8\"], [\"h=1.00\"]], operator_name = \"(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)\", scale_y = 'linear', separator = '\\t')\n",
    "# sigma x prob dist all sectors\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSpecRapSigmaXAllSectors\",columns = [(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)],\n",
    " #distinguishers = [], operator_name = \"P(r) for all sectors\", scale_y = 'linear', separator = '\\t\\t') \n",
    "\n",
    "# energy difference\n",
    "#plot_av_operator_dfs(symm_pbc + \"/EnergyDiff/\", \"perturbationEnergyDiffDist\",columns = [(\"E-E'\", 0), (\"P(E-E')\",1)], \n",
    "#                     distinguishers = [[\"L=18\"],[\"g=0.80\"], [\"h=1.80\"], [\"pert=0.0100\", \"pert=0.0300\", \"pert=0.1100\", \"pert=0.1500\"]],\n",
    "#                     operator_name = \"P(E-E')\", scale_y = 'log', fitfunctions =[], xmin = -0.25, xmax = 0.25) \n",
    "# operators dist\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "593c543b-9fca-4021-ab68-245f4c2a06f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_8824/4091695942.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\maxgr\\AppData\\Local\\Temp/ipykernel_8824/4091695942.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    #makegif(symm_pbc, \"perturbationOperatorsDist$S^x$\", [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"]], images)\u001b[0m\n\u001b[1;37m                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for hval in hvalues:    \n",
    "    images = []\n",
    "    for i in pert_vec:\n",
    "        #print(i)\n",
    "        #im = plot_av_operator_dfs(symm_pbc, \"perturbationOperatorsDist\",columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)],\n",
    "                        #distinguishers = [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"], [f\"pert={i:.4f}\"]],\n",
    "                        #operator_name = \"Probability distribution with perturbation\", scale_y =\"log\",\\\n",
    "                         #fitfunctions=[laplace_fit, gauss_fit],gif=True,save_files = False,\\\n",
    "                         #xmin = -0.1, xmax = 0.1, ymin = 0.1, ymax = 500) \n",
    "        #images.append(im)     \n",
    "    #makegif(symm_pbc, \"perturbationOperatorsDist$S^x$\", [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"]], images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0121b81b-2c01-4cf3-b16e-c3717d365438",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_8824/3500389758.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\maxgr\\AppData\\Local\\Temp/ipykernel_8824/3500389758.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    distinguishers = [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"], [f\"pert={i:.4f}\"]],\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in pert_vec[5:17]:\n",
    "    print(i)\n",
    "    images = []\n",
    "    for hval in hvalues:    \n",
    "        \n",
    "        print(hval)\n",
    "        #im = plot_av_operator_dfs(symm_pbc, \"perturbationOperatorsDist\",columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)],\n",
    "                        distinguishers = [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"], [f\"pert={i:.4f}\"]],\n",
    "                        operator_name = \"Probability distribution with perturbation\", scale_y =\"log\",\\\n",
    "                         fitfunctions=[laplace_fit, gauss_fit],gif=True,save_files = False,\\\n",
    "                         xmin = -0.1, xmax = 0.1, ymin = 0.1, ymax = 500)\n",
    "        if(len(im)>0):\n",
    "            images.append(im)\n",
    "        \n",
    "    makegif(symm_pbc, \"perturbationOperatorsDist$S^x$\", [[\"L=18\"],[\"g=0.80\"], [f\"pert={i:.4f}\"]], images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ipr scaling with h\n",
    "#plot_av_operator_dfs(symm_pbc, \"IprScaling\",columns = [(\"h\", 0), (\"ipr\", 1), (\"Information entropy\", 2), (\"$r_{goe}$\",3)], distinguishers = [[\"L=18\"],[\"g=1.20\",\"g=0.80\", \"g=0.40\"]], operator_name = \"System scaling with h\")\n",
    "# moments of probability sigma_x fluct\n",
    "#plot_av_operator_dfs(symm_pbc, \"Moments\",columns = [(\"h\", 0), (\"$U_B$\", 1), (\"Kurtossis\", 2), (\"$\\sigma$\",3)], distinguishers = [[\"L=18\"],[\"g=1.20\",\"g=0.80\"]], operator_name = \"System scaling with h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd041cf-f1a3-438a-aea7-a047b120ed1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca04311-6f9e-4eba-b9f5-c444d26bef4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot only distributions in different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08422308-a8a9-4bea-8af9-638848300620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(directory, file_begin, columns, distinguishers, distribution_name, column_num = [], separator = \"\\t\\t\"):\n",
    "    try:\n",
    "        folder = 'plots/' + directory \n",
    "        if not os.path.isdir(folder):\n",
    "            os.makedirs(folder)\n",
    "    except OSError:\n",
    "        print(\"Creation of the directory %s failed\" % (folder))\n",
    "    else:\n",
    "        sns.set_style(\"ticks\")\n",
    "        # find the list of dataframes\n",
    "        listOfDf = handle_all_files(directory, file_begin,columns, distinguishers, column_num, separator)\n",
    "        labels = [item[1] for item in listOfDf]\n",
    "        same_and_diff = findDifferentElementsStrings(labels)\n",
    "        \n",
    "        # plot \n",
    "        counter = 0\n",
    "        for df in listOfDf:\n",
    "            savefile = file_begin + \"_\" + same_and_diff[0] + \",\" + labels[counter]\n",
    "            fig, ax = plt.subplots(len(columns) - 1, figsize=(12,8), dpi = 120) \n",
    "            axis = []\n",
    "            if len(columns) > 2: # if we have many columns to plot\n",
    "                axis = [ax[i] for i in range(len(columns) - 1)]\n",
    "            else:\n",
    "                axis = [ax]\n",
    "            \n",
    "            step = 0.01\n",
    "            _min = np.min(df[0].index)\n",
    "            _max = np.max(df[0].index)\n",
    "            new_ticks = [_min + step * i for i in range(0, int((_max-_min)/step) + 1)]\n",
    "            print(new_ticks)\n",
    "            itr = 0\n",
    "            for col in df[0].columns:      \n",
    "\n",
    "                df[0][col].plot.bar(ax = axis[itr], rot=15, color = next(colors))\n",
    "                axis[itr].set(title = distribution_name + \" for \" + same_and_diff[0] + \",\" + same_and_diff[1][counter]\n",
    "                            ,xlabel = col\n",
    "                            ,ylabel = distribution_name)\n",
    "                #axis[itr].get_legend().remove()\n",
    "                #(same_and_diff\n",
    "                #            , frameon=False\n",
    "                #            , loc='best'\n",
    "                #            , fontsize=8)\n",
    "                axis[itr].set_xticks(np.interp(new_ticks, df[0].index, np.arange(df[0].size)))\n",
    "                axis[itr].set_xticklabels(new_ticks)\n",
    "                itr+=1\n",
    "            counter+=1\n",
    "            plt.savefig(folder + savefile + \".pdf\")\n",
    "            plt.savefig(folder + savefile + \".png\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#plot_distribution(symm_pbc, \"ProbDistSpecRapSigmaX\", columns = [\"P(|r|)\", \"<n + 1|$S_x$|n + 1> - <n|$S_x$|n>\"], distinguishers = [], distribution_name= \"P(|r|)\", separator = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e3eea-5a41-4a33-87fb-ae7c493e828f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PLOT HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d40c566d-6ddf-4ca5-aca6-0757a23e222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_map(destination, filename, columns, separator = \"\\t\"):\n",
    "    # save in folder\n",
    "    df = pd.read_csv(f\"{destination}{filename}\", sep = separator, header=None)\n",
    "    df = df.dropna(axis = 1)\n",
    "    #mapka\n",
    "    df.columns = columns\n",
    "    #return df    \n",
    "    #df.set_index([columns[0], columns[1]], inplace = True)\n",
    "    #cols = df.columns\n",
    "    #df[cols[0]] = df[cols[0]].replace('  nan',0)\n",
    "    \n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,10), dpi = 100)\n",
    "    savefile = f\"{destination}{filename[0:-4]}.pdf\"\n",
    "\n",
    "    #print(cols)\n",
    "\n",
    "    #pivotted = df.pivot(cols[0])\n",
    "    # PLOT \n",
    "    z = np.array(df.loc[:,columns[2]])\n",
    "    z = np.array([0 if i == '  nan' else float(i) for i in z])\n",
    "    #print(z)\n",
    "    x = np.array(df.loc[:,columns[0]])\n",
    "    x = np.unique(x)\n",
    "    print(x)\n",
    "    y = np.array(df.loc[:,columns[1]])\n",
    "    y = np.unique(y)\n",
    "    print(y)\n",
    "\n",
    "    X,Y = np.meshgrid(y,x)\n",
    "\n",
    "    Z=z.reshape(len(x),len(y))\n",
    "\n",
    "    heatmap = plt.pcolormesh(X,Y,Z,cmap='coolwarm',vmin=np.min(z), vmax=1 )\n",
    "    #pivotted = df.pivot(x,y,z)\n",
    "    #print(pivotted)\n",
    "    #\n",
    "    #mini = np.min(z)\n",
    "    #maxi = np.max(z)\n",
    "    ##print(mini,maxi)\n",
    "    #sns.heatmap(X,Y,Z,cmap='coolwarm',vmin=np.min(z), vmax=np.max(z))\n",
    "    ##sns.heatmap(pivotted,cmap='coolwarm',vmin=np.min(z), vmax=np.max(z))\n",
    "    if(len(columns) == 3):\n",
    "        plt.xlabel(columns[1])\n",
    "        plt.ylabel(columns[0])\n",
    "        \n",
    "    ##ax.set_zscale([0.35,0.535])\n",
    "    plt.title(f\"{columns[2]} for different lattent dimensions and transverse field disorder strength\\n\\\n",
    "    $δg$ in (g+δg) * $S_x$\")\n",
    "    #plt.legend()\n",
    "    fig.colorbar(heatmap)\n",
    "    plt.savefig(savefile)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cd8192e-06a6-4aa9-8955-03d3fc3885fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'serif' not found because none of the following families were found: Times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.3 0.5 0.7 0.9 1.1 1.3 1.5 1.7 1.9 2.1 2.3 2.5 2.7 2.9]\n",
      "[0.00488 0.11523 0.22559 0.33643 0.44678 0.55762 0.66797 0.77881 0.88916\n",
      " 1.     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'serif' not found because none of the following families were found: Times\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "destination = f\"C:{kPSep}Users{kPSep}maxgr{kPSep}Desktop{kPSep}Wyniken{kPSep}WAVES{kPSep}\"\n",
    "plot_heat_map(destination, \"heatmap_layernum=2.txt\", [\"δg\",\"compression C=lat_dim / $2^L$\", \"fidelity (bhattacharya coefficient)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89b0c4-3fee-4a40-b144-f99f110236b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73547c54-f0b3-4201-8229-ce866a6aad5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25943d8c-990d-41ac-8594-f3c46ec46639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa39582-2089-4549-9651-3f6692631880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6645a81-0275-4e69-b6ad-d2a617f2f65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae0cec-3e53-4d45-980e-9d279dac0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d37cb-be13-4aab-aa22-6c7bf45e5953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278057ef-b759-423f-8c12-5e018638c1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535167d-c40e-403d-b3c7-a738e1512c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd16f82-0245-49f3-959e-86170417b833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df36e6-63aa-46a3-9e5f-75fb447b8f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604d88f-957d-400a-8bb4-4a05d7a88033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d7ab51-a9e6-49ed-b446-057f4f38d6fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16bc8e1b-a96a-457c-9e2e-f017c0357da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationEntropy(stateA, statesB, N, sqrt = False):\n",
    "    ent = 0\n",
    "    for stateB in statesB:\n",
    "        c_k = 0\n",
    "        if not sqrt:\n",
    "            c_k = np.dot(stateA, (stateB))\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                c_k += math.sqrt(stateA[i]) *math.sqrt(stateB[i])\n",
    "                \n",
    "        val = abs(c_k)\n",
    "        ent += val * math.log(val)\n",
    "    return -ent / math.log(0.48 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54959608-1c84-46c5-b789-269de2f5442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7559289460184544"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def theirFidelity(probaA, probaB):\n",
    "    # first state\n",
    "    mini, maxi = np.min(probaA), np.max(probaA)\n",
    "    #print(mini,maxi)\n",
    "    h =2* (np.quantile(probaA,0.75) - np.quantile(probaA,0.25)) * math.pow(len(probaA),-1/2)\n",
    "    bins = [mini + i * h for i in range(int((maxi - mini)/h))]\n",
    "    #print(bins)\n",
    "    \n",
    "    f1, b1 = np.histogram(probaA, density=True, bins=bins)\n",
    "    #plt.hist(probaA, density=True, bins=b1)\n",
    "    # second state\n",
    "    f2, b2= np.histogram(probaB, density = True, bins = b1)\n",
    "    #plt.hist(probaB, density=True, bins=b1)\n",
    "    \n",
    "    f1/=f1.sum()\n",
    "    f2/=f2.sum()\n",
    "    return np.sum(np.sqrt(np.multiply(f1, f2)))\n",
    "\n",
    "probaA = np.array([0.3602150537634409, 0.42028985507246375, \n",
    "  0.373117033603708, 0.36813186813186816, 0.32517482517482516, \n",
    "  0.4175257731958763, 0.41025641025641024, 0.39408866995073893, \n",
    "  0.4143222506393862, 0.34, 0.391025641025641, 0.3130841121495327, \n",
    "  0.35398230088495575])\n",
    "probaA /= np.sum(probaA)\n",
    "\n",
    "probaB = np.array([0.3602150537634409, 0.42028985507246375, \n",
    "  0.373117033603708,\n",
    "  0.463, 0.41025641025641024, 0.39408866995073893, \n",
    "  0.406393862, 0.34, 0.391025641025641, 0.3130841121495327, \n",
    "  0.35398230088495575])\n",
    "probaB /= np.sum(probaB)\n",
    "\n",
    "\n",
    "theirFidelity(np.array(probaA), np.array(probaB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37594429-39c0-4b9e-be57-7574d83a3510",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "250827d2-568f-46a8-8795-53550ac5d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5389786863362994243\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5060693856\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16378740401165130101\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n",
    "from keras.callbacks import History\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "from keras import Input, Model\n",
    "from keras import regularizers\n",
    "from keras import initializers, optimizers\n",
    "from plot_keras_history import plot_history\n",
    "from tqdm.keras import TqdmCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, Rescaling\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6ba52a2-a8c6-41fe-b8ae-f4fd64b3b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347ef2d-277d-4aef-8109-5e5988ed61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e452f84-e6a2-43f6-b464-b395ad01403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f3883-23cc-4015-9279-475b0cee0bba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AUTOENCODER KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73370b5e-3551-48b1-a9ca-44c7199fb92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dca4002-4444-4e4a-81a1-8db0ccfc0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e955c3a-1ede-4e17-86de-b2ab3a5e5a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b60a9e5-9fc7-47ca-b726-d1be3892de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoEnc(latent_dim, shape, nlayers = 1, alfa = 0.1): \n",
    "    print(f\"creating {nlayers} with input shape {shape}\")\n",
    "    input_size = shape[1]\n",
    "    compression = int(input_size * latent_dim / input_size)\n",
    "    reduction = compression // nlayers\n",
    "    if input_size - (nlayers-1)*reduction <= 0:\n",
    "        print(f\"to many layers {nlayers} for compression {compression}\")\n",
    "    \n",
    "    \n",
    "    # ------ encoder\n",
    "    inputer = None\n",
    "    encoder = None\n",
    "    if nlayers == 1:\n",
    "        inputer = Input(input_size)\n",
    "        #shap = (None \n",
    "        encoder = layers.Dense(input_size - (nlayers - 1)*reduction,\\\n",
    "                       activation=keras.activations.sigmoid)(inputer)\n",
    "        # we must have size bigger than 0 so check maximum for those number of layers\n",
    "    else:\n",
    "        # add linear layer\n",
    "        inputer = Input(input_size)\n",
    "        encoder = layers.Dense(input_size - reduction,\\\n",
    "                                   activation = layers.LeakyReLU(alpha = alfa))(inputer)\n",
    "        # add leaky relus\n",
    "        counter = 2\n",
    "        for i in range(nlayers - 3):\n",
    "            encoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                                   activation = layers.LeakyReLU(alpha = alfa))(encoder)\n",
    "            counter += 1\n",
    "        \n",
    "        # add last dense layer\n",
    "        encoder = layers.Dense(input_size - (nlayers - 1)*reduction,\\\n",
    "                               activation=keras.activations.sigmoid)(encoder)\n",
    " \n",
    "    \n",
    "    # --------- Latent log variance and mu layers    \n",
    "    fc_logvar = layers.Dense(latent_dim, name = \"log_var\")(encoder)\n",
    "    fc_mu = layers.Dense(latent_dim, name = \"mean\")(encoder)\n",
    "    fc = Sampling()([fc_mu, fc_logvar])\n",
    "    encod = keras.Model(inputer, [fc_mu, fc_logvar, fc], name=\"encoder\")\n",
    "    encod.summary()\n",
    "    \n",
    "    \n",
    "    # ------ decoder\n",
    "    inputer = keras.Input(latent_dim)\n",
    "    decoder = None\n",
    "    if nlayers == 1:\n",
    "        decoder = layers.Dense(input_size, activation = keras.activations.sigmoid)(inputer)\n",
    "        # we must have size bigger than 0 so check maximum for those number of layers\n",
    "    else:\n",
    "        counter = nlayers - 1\n",
    "        decoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                               activation = layers.LeakyReLU( alpha = alfa))(inputer)\n",
    "        \n",
    "        counter -= 1\n",
    "        for i in range(nlayers - 2):\n",
    "            decoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                                   activation = layers.LeakyReLU( alpha = alfa))(decoder)\n",
    "            counter -= 1\n",
    "\n",
    "        # add last dense layer\n",
    "        decoder = layers.Dense(input_size, activation=keras.activations.sigmoid)(decoder)        \n",
    "    \n",
    "    decod = keras.Model(inputer, decoder, name=\"decoder\")\n",
    "    decod.summary()\n",
    "    \n",
    "    return encod,decod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86df192-bd99-473d-88bb-762d86fbec45",
   "metadata": {},
   "source": [
    "### Variational nn class keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f633fa7c-54bb-48de-bcec-0dddc0d3e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder,epochs, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        # LOG AND MEAN ARE IN THE ENCODER ALREADY\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            weight = 0.85 * (self.epoch / self.epochs)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss * weight  #* self.weight\n",
    "        \n",
    "            #print(f\"with weight = {self.weight} and {total_loss}\")    \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    def scheduler(self, epoch, lr):\n",
    "\n",
    "        self.epoch = epoch\n",
    "\n",
    "        #print(self.weight)\n",
    "        return lr#self.weight\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2759c4-42c7-48fe-b408-aeb12c5b141c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AUTOENCODER PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66760d64-dbce-4bc5-b66f-896df911939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Print(nn.Module):\n",
    "    \"\"\"\n",
    "    Used to print the shape of the data inside of nn.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Print, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "278c9d46-ea1a-4274-b377-769d6ff2ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(input_size, n_layers, compression):\n",
    "    \n",
    "    reduction = compression // n_layers\n",
    "    encoder = None\n",
    "    # These if statements determine the structure of the VAE dependent on the desired compression\n",
    "    print(input_size, reduction)\n",
    "    if n_layers == 1:\n",
    "        # Desired structure for 1 total layer in the encoder\n",
    "        encoder = nn.Sequential(nn.Sigmoid())\n",
    "    elif n_layers == 2:\n",
    "        # Desired structure for 2 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 3:\n",
    "        # Desired structure for 3 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 4:\n",
    "        # Desired structure for 4 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 5:\n",
    "        # Desired structure for 5 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 6:\n",
    "        # Desired structure for 6 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 7:\n",
    "        # Desired structure for 7 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    decoder = None\n",
    "\n",
    "    if n_layers == 7:\n",
    "        # Desired structure for 7 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 6,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif (n_layers == 6):\n",
    "        # Desired structure for 6 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif (n_layers == 5):\n",
    "        # Desired structure for 5 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 4:\n",
    "        # Desired structure for 4 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 3:\n",
    "        # Desired structure for 3 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 2:\n",
    "        # Desired structure for 2 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 1:\n",
    "        # Desired structure for 1 total layer in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Latent log variance and mu layers\n",
    "    fc_logvar = nn.Linear(input_size - reduction * (n_layers - 1), compression)\n",
    "    fc_mu = nn.Linear(input_size - reduction * (n_layers - 1), compression)\n",
    "\n",
    "\n",
    "\n",
    "    return {'decoder': decoder, 'encoder': encoder, 'logvar': fc_logvar, 'mu': fc_mu}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72c735-29c2-4065-9d3b-8ba5bcb7a1c3",
   "metadata": {},
   "source": [
    "### load file from folder like dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "538dec57-bde8-434a-a487-863abdbc2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own class LoadFromFolder\n",
    "class FromFolder(Dataset):\n",
    "    def __init__(self, main_dir, filenum = None):\n",
    "         \n",
    "        # Set the loading directory\n",
    "        self.main_dir = main_dir\n",
    "        # List all images in folder and count them\n",
    "        all_files = list(filter(lambda x: x.endswith('.dat'), os.listdir(self.main_dir)))\n",
    "        if filenum != None:\n",
    "            all_files = all_files[0:filenum]\n",
    "        self.total_files = sorted(all_files)\n",
    "    def __len__(self):\n",
    "        # Return the previously computed number of images\n",
    "        return len(self.total_files)   \n",
    "    def __getitem__(self, idx):\n",
    "        loc = os.path.join(self.main_dir, self.total_files[idx])\n",
    "        fil = np.square(np.genfromtxt(loc))       \n",
    "        return fil\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54becccd-56da-422a-980a-2c5dee46d181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model class in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4da2272c-abd5-4d55-90b0-a23ef62fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelik:\n",
    "    def __init__(self, parameters, data_path, lat_dim, verbosity=0, n_layers=3, n_qubits=8,trainsize=0.9,filenum = None, load=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parameters: dict of json params\n",
    "            n_layers: number of layers in the encoder/decoder\n",
    "            n_qubits: number of lattice sites\n",
    "            load: optional path to load a pretrained model\n",
    "        \"\"\"\n",
    "        # Initialize class parameteres\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = n_qubits\n",
    "        self.N = int(math.pow(2, self.n_qubits))\n",
    "        self.lat_dim = lat_dim\n",
    "        self.compression = self.lat_dim / self.N\n",
    "        \n",
    "        self.epochs = int(parameters['epochs'])\n",
    "        self.batch_size = int(parameters['batch_size'])\n",
    "        self.trainsize = trainsize\n",
    "        self.display_epochs = int(parameters['display_epoch'])\n",
    "        self.learning_rate = parameters['learning_rate']\n",
    "        self.num_batches = int(parameters['num_batches'])\n",
    "        self.data_path = data_path\n",
    "        self.filenum = filenum\n",
    "\n",
    "\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        self.savename = self.data_path + f'..{kPSep}myModel_latent={self.lat_dim},{self.N}'\n",
    "\n",
    "        # Prepare model\n",
    "        self.vae, self.train_loaders, self.test_loaders, self.optimizer = self.prepare_model(\n",
    "            load=load)\n",
    "\n",
    "        # Train the model if it wasn't loaded, and compute fidelity\n",
    "        if load == None:\n",
    "            train_losses, test_losses = self.run_model()\n",
    "            self.plot_losses(train_losses, test_losses)\n",
    "        \n",
    "        \n",
    "        #print(self.train_loaders.dataset)\n",
    "        self.fidelity = self.get_fidelity(self.train_loaders)\n",
    "\n",
    "    def prepare_model(self, load=None):\n",
    "        \"\"\"\n",
    "        Initializes VAE model and loads it onto the appropriate device.\n",
    "        Reads and loads the data in the form of an array of Torch DataLoaders.\n",
    "        Initializes Adam optimizer.\n",
    "        Args:\n",
    "            load: path to load trained model from\n",
    "        Returns:\n",
    "            VAE\n",
    "            Array of train Torch Dataloaders\n",
    "            Array of test Torch Dataloaders\n",
    "            Adam optimizer\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        input_size = self.N\n",
    "        VAE_layers = get_layers(input_size, self.n_layers, self.lat_dim)\n",
    "        \n",
    "        vae = VariationalAutoencoder(VAE_layers.get('encoder'), VAE_layers.get(\n",
    "            'decoder'), VAE_layers.get('logvar'), VAE_layers.get('mu')).double().to(self.device)\n",
    "        \n",
    "        train_loaders, test_loaders = self.get_data(self.batch_size, self.data_path)\n",
    "    \n",
    "        optimizer = optim.Adam(vae.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        if not load == None:\n",
    "            vae.load_state_dict(torch.load(load))\n",
    "            vae.eval()\n",
    "\n",
    "        return vae, train_loaders, test_loaders, optimizer\n",
    "\n",
    "    def loss_function(self, x, x_reconstruction, mu, log_var, weight=1):\n",
    "        \"\"\"\n",
    "        Returns the loss for the model based on the reconstruction likelihood and KL divergence\n",
    "        Args:\n",
    "            x: Input data\n",
    "            x_reconstruction: Reconstructed data\n",
    "            mu:\n",
    "            log_var:\n",
    "            weight:\n",
    "        Returns:\n",
    "            loss:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        reconstruction_likelihood = F.binary_cross_entropy(\n",
    "            x_reconstruction, x, reduction='sum')\n",
    "        kl_divergence = -0.5 * \\\n",
    "                        torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        loss = reconstruction_likelihood + kl_divergence * weight\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_fidelity(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the reconstruction fidelity.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data\n",
    "        Returns:\n",
    "            out: Fidelity for the input sample\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        #self.vae.eval()\n",
    "        #torch.no_grad()\n",
    "        fidel = 0\n",
    "        counter = 0\n",
    "\n",
    "        x = x.dataset\n",
    "        #x = x.dot(1 << np.arange(x.shape[-1] - 1, -1, -1))  # Converts binary string to integer\n",
    "        print(x)\n",
    "        \n",
    "        l, u = x.min(), x.max() + 1\n",
    "        f1, b = np.histogram(x, density=True, bins=np.arange(l, u, 1))\n",
    "        \n",
    "        # Initialize for getting reconstructed density\n",
    "        f2 = np.zeros(f1.shape)\n",
    "        ns = 0\n",
    "        dim = int(self.n_qubits * self.compression)\n",
    "        while ns < 10:\n",
    "            # Get samples, decode them, convert to int, and add to hist count\n",
    "            re = np.random.multivariate_normal(\n",
    "                np.zeros(dim), np.eye(dim), size=int(0.375e7))\n",
    "            re = self.vae.decode(torch.Tensor(re).double().to(\n",
    "                self.device)).cpu().detach().numpy()\n",
    "            f2 += np.histogram(re, bins=b)[0]\n",
    "            ns += 1\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #    for i, data in enumerate(x):\n",
    "        #        if i >= self.num_batches:\n",
    "        #            break\n",
    "#\n",
    "        #        #print(data)\n",
    "        #        tmp = data.to(self.device)\n",
    "        #        reconstruction_data, mu, logvar = self.vae(tmp)\n",
    "        #        reconstruction_data = reconstruction_data.cpu()\n",
    "        #        #print(reconstruction_data)\n",
    "        #        for e in range(len(reconstruction_data)):\n",
    "        #            a = reconstruction_data[e].numpy()\n",
    "        #            b = data[e].numpy()\n",
    "        #            fidel += theirFidelity(a, b)\n",
    "        #            counter+=1\n",
    "        \n",
    "        db = np.array(np.diff(b), float)\n",
    "        f2 = f2 / db / f2.sum()\n",
    "\n",
    "        out = np.sum(np.sqrt(np.multiply(f1, f2)))\n",
    "        print(f\"Fidelity: {out}\")\n",
    "        del re, x_re, f1, f2, x\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return out #fidel/counter\n",
    "\n",
    "    def train(self, epoch, loader):\n",
    "        \"\"\"\n",
    "        Trains the VAE model\n",
    "        Args:\n",
    "            epoch: Number of current epoch to print\n",
    "            loader: Torch DataLoader for a quantum state\n",
    "        Returns:\n",
    "            epoch_loss: Loss for the epoch\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        self.vae.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, data in enumerate(loader):\n",
    "\n",
    "            if i >= self.num_batches:\n",
    "                break\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            reconstruction_data, mu, log_var = self.vae(data)\n",
    "            \n",
    "            loss = self.loss_function(\n",
    "                data, reconstruction_data, mu, log_var, weight=0.85 * (epoch / self.epochs))\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item() / (data.size(0) * self.num_batches)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (self.verbosity == 0 or (\n",
    "                    self.verbosity == 1 and (epoch + 1) % self.display_epochs == 0)) and i % self.batch_size == 0:\n",
    "                print(\"Done batch: \" + str(i) +\n",
    "                      \"\\tCurr Loss: \" + str(epoch_loss))\n",
    "\n",
    "        if self.verbosity == 0 or (self.verbosity == 1 and (epoch + 1) % self.display_epochs == 0):\n",
    "            print('Epoch [{}/{}]'.format(epoch + 1, self.epochs) +\n",
    "                  '\\tLoss: {:.4f}'.format(epoch_loss)\n",
    "                  )\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def test(self, epoch, loader):\n",
    "        \"\"\"\n",
    "        Tests VAE model\n",
    "        Args:\n",
    "            epoch: Number of current epoch to print\n",
    "            loader: Torch DataLoader for a quantum state\n",
    "        Returns:\n",
    "            epoch_loss: Loss for the epoch\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        self.vae.eval()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(loader):\n",
    "\n",
    "                if i >= self.num_batches:\n",
    "                    break\n",
    "\n",
    "                data = data.to(self.device)\n",
    "                reconstruction_data, mu, logvar = self.vae(data)\n",
    "                loss = self.loss_function(\n",
    "                    data, reconstruction_data, mu, logvar)\n",
    "                epoch_loss += loss.item() / (data.size(0) * self.num_batches)\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def run_model(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: Quantum state the model will be trained on\n",
    "        Returns:\n",
    "            test and training loss\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader, test_loader = self.train_loaders, self.test_loaders\n",
    "        train_losses, test_losses = [], []\n",
    "\n",
    "        print(\"Beginning Training:\")\n",
    "        for e in range(0, self.epochs):\n",
    "            train_loss = self.train(e, train_loader)\n",
    "            test_loss = self.test(e, test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Final train loss: {train_loss}\\tFinal test loss: {test_loss}\")\n",
    "\n",
    "        #torch.save(self.vae.state_dict(),self.savename)\n",
    "\n",
    "        return train_losses, test_losses\n",
    "\n",
    "    def plot_losses(self, train_losses, test_losses):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_losses: list of training losses from run_model\n",
    "            test_losses: list of testing losses from run_model\n",
    "        Returns:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        savename=self.savename\n",
    "        epochs = np.arange(0, len(train_losses), 1)\n",
    "        plt.plot(epochs, train_losses, \"g-\", label=\"Training Loss\")\n",
    "        plt.plot(epochs, test_losses, \"b-\", label=\"Testing Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"VAE Training Loss with {self.n_layers} layers\")\n",
    "        plt.legend()\n",
    "        plt.xlim(0, len(train_losses))\n",
    "        \n",
    "        figure_num = 1\n",
    "        while os.path.exists(f'{savename}_loss_{figure_num}.png'):\n",
    "            figure_num += 1\n",
    "        plt.savefig(f'{savename}_loss_{figure_num}.png')\n",
    "        plt.clf()\n",
    "        print(f'{savename}_loss_{figure_num}.png')\n",
    "\n",
    "    def plot_fidelities(self, fs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fs: A list of Fidelities from each model\n",
    "        Returns:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        \n",
    "        savename=self.savename\n",
    "        epochs = np.arange(1, len(fs) + 1, 1)\n",
    "        plt.plot(epochs, fs, \"b--o\", label=\"Fidelity\")\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.xticks(ticks=epochs)\n",
    "        plt.ylabel(\"Fidelity\")\n",
    "        plt.title(\"VAE Fidelities\")\n",
    "        plt.xlim(epochs.min(), epochs.max())\n",
    "        \n",
    "        \n",
    "        figure_num = 1\n",
    "        while os.path.exists(f'{savename}_fidelity_{figure_num}.png'):\n",
    "            figure_num += 1\n",
    "        plt.savefig(f'{savename}_fidelity_{figure_num}.png')\n",
    "        plt.clf()\n",
    "        print(f'{savename}_fidelity_{figure_num}.png')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_data(self, batch_size, file_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: Size of batches\n",
    "            file_path: Path of file location\n",
    "        Returns:\n",
    "            train_loaders: Array of Torch DataLoaders representing quantum states for training\n",
    "            test_loaders: Array of Torch DataLoaders representing quantum states for testing\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        ds = FromFolder(file_path, self.filenum)\n",
    "        num = ds.__len__()\n",
    "        num_train = int(num * self.trainsize)\n",
    "        train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(ds, (num_train, num - num_train, 0))\n",
    "        \n",
    "        train_loaders  = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loaders  = torch.utils.data.DataLoader(\n",
    "            valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc43c8-49d3-4b92-ac03-20f64f649386",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variational nn class pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "79ceb59c-7c21-4557-888b-e900b1ffccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder class.\n",
    "    Architecture:\n",
    "        - x Fully connected layers\n",
    "        - Sigmoid activation function\n",
    "        - LeakyReLU activation function with slope of -0.2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encode, decode, logvar, mu):\n",
    "        \"\"\"\n",
    "        Very standard VAE, all the heavy lifting done elsewhere\n",
    "        Args:\n",
    "            encode: encoder input from hidden_layers\n",
    "            decode: decoder layers\n",
    "            logvar: logvar layer\n",
    "            mu:mu layer\n",
    "        \"\"\"\n",
    "\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.LReLU = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fc_logvar = logvar\n",
    "        self.fc_mu = mu\n",
    "\n",
    "        self.encode = encode\n",
    "        self.decode = decode\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.encode(x)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return reconstruction, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ce0a4-1e6c-40a2-8cf1-8d147acde623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b2c8dd8-f351-4f38-bc50-561a482f5054",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d942a082-fe85-430d-9b2a-1c95b8dfe4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileAutoencode(directory, model, latent_dim, epo,layer_num, trainsize = 0.8,filenum = None, batch = 10, verbose = 2,\\\n",
    "                   activation = 'relu', savename = 'model', save = True, savefiles = False):\n",
    "    folderLog = directory + \"_\" + model.getInfo() + kPSep\n",
    "    folder = folderLog + \"wavefunctions\" + kPSep\n",
    "    folderSaveNew = folderLog + \"wavefunctions_encoder\" + kPSep\n",
    "    createFolder([folderSaveNew])\n",
    "    savename = folderLog + f'myModel_latent={latent_dim/model.N:.3f}'\n",
    "    # read files\n",
    "    wavefuns = []\n",
    "    \n",
    "    files = list(filter(lambda x: x.endswith('.dat'), os.listdir(folder)))\n",
    "    maximum = len(files)\n",
    "    if filenum != None:\n",
    "        maximum = filenum\n",
    "    \n",
    "\n",
    "    counter = 0\n",
    "    # create squares of the wavefunctions\n",
    "    for filename in files:\n",
    "        tmp = np.square(np.genfromtxt(folder+filename))\n",
    "        wavefuns.append(tmp)\n",
    "        if counter == maximum:\n",
    "            break\n",
    "        counter+=1\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(wavefuns, wavefuns, test_size=1-trainsize, shuffle= True)\n",
    "    tmp = np.concatenate([x_train, y_train])\n",
    "    \n",
    "    #print(x_train, y_train)\n",
    "    # separate training etc\n",
    "    #data_train = np.array(wavefuns[0:int(trainsize*maximum)])\n",
    "    #data_test = np.array(wavefuns[int(trainsize*maximum):])\n",
    "    #print(data_test)\n",
    "    \n",
    "    print(f'\\n\\n\\t\\tMaking autoencoder with latent={latent_dim/model.N}\\t\\t\\n\\n')    \n",
    "    encoder, decoder = autoEnc(latent_dim, (None, int(modelik.N)),layer_num, alfa = 0.2)\n",
    "    vae = VAE(encoder, decoder, epo)\n",
    "    # first model compile\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(lr=1e-3))\n",
    "    vae.build((None, int(modelik.N)))\n",
    "    \n",
    "    #story = vae.fit(tmp,\n",
    "    #epochs=1,\n",
    "   #shuffle=False,\n",
    "    #batch_size = batch,\n",
    "    #verbose=0)\n",
    "    \n",
    "    vae.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # making callbacks\n",
    "    early_stopping_cb = callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "    callback = [early_stopping_cb]\n",
    "    if save:\n",
    "        callback.append(keras.callbacks.ModelCheckpoint(savename + \".h5\", save_best_only=True))\n",
    "    if verbose == 2:\n",
    "        callback.append(TqdmCallback(verbose = verbose))\n",
    "    \n",
    "    # define number of iterations in training and test\n",
    "    train_iter = round(np.array(x_train).shape[0]/batch)\n",
    "    test_iter = round(np.array(x_valid).shape[0]/batch)\n",
    "    \n",
    "    # make artificial history\n",
    "    history = {}\n",
    "    history['history'] = {}\n",
    "    history['history']['total_loss'] = []\n",
    "    history['history']['reconstruction_loss'] = []\n",
    "    history['history']['kl_loss'] = []   \n",
    "    \n",
    "    tot_loss, reconstr_loss, kl_loss = 0, 0, 0\n",
    "    for ep in range(epo):\n",
    "        vae.epoch = ep\n",
    "        vae.compile(optimizer=keras.optimizers.Adam(lr=1e-3))\n",
    "        # train iterations\n",
    "        for i in range(train_iter):\n",
    "            start = i*batch\n",
    "            end = i*batch + batch\n",
    "            batchX = np.array(x_train[start:end]).astype('float32')\n",
    "            \n",
    "            #batchY = np.array(y_train[start:end]).astype('float32')\n",
    "            #print(batchX.shape)\n",
    "            #tmp = np.concatenate([batchX, batchY])\n",
    "            #tmp = np.expand_dims(tmp, -1).astype(\"float32\")\n",
    "            print(batchX, batchX.shape)\n",
    "\n",
    "           #story = vae.fit(tmp,\n",
    "           #  epochs=1,\n",
    "           #  shuffle=False,\n",
    "           #  batch_size = batch,\n",
    "           #  verbose=0,\n",
    "           #  callbacks = callback)\n",
    "            tot_loss_,reconstr_loss_,kl_loss_ = vae.train_on_batch(batchX)\n",
    "            #print(dic)\n",
    "            #tot_loss_ = dic['loss']\n",
    "            #reconstr_loss_ = dic['reconstruction_loss'] \n",
    "            #kl_loss_ = dic['kl_loss']\n",
    "\n",
    "            #print(tot_loss_, reconstr_loss_, kl_loss_)\n",
    "            tot_loss += tot_loss_\n",
    "            reconstr_loss += reconstr_loss_\n",
    "            kl_loss += kl_loss_\n",
    "        history['history']['total_loss'].append(tot_loss/train_iter)\n",
    "        history['history']['reconstruction_loss'].append(reconstr_loss/train_iter)\n",
    "        history['history']['kl_loss'].append(kl_loss/train_iter)    \n",
    "        \n",
    "        # test iterations \n",
    "        #val_tot_loss, val_reconstr_loss, val_kl_loss = 0, 0, 0\n",
    "        #for i in range(test_iter):\n",
    "        #    start = i*batch\n",
    "        #    end = i*batch + batch\n",
    "        #    batchX = np.array(x_valid[start:end]).astype('float32')\n",
    "        #    batchY = np.array(y_valid[start:end]).astype('float32')\n",
    "        #    \n",
    "        #    tmp = np.concatenate([batchX, batchY], axis = 0)\n",
    "        #    #tmp = np.expand_dims(tmp, -1).astype(\"float32\")\n",
    "        #    #istory = vae.fit(tmp,\n",
    "        #    #   epochs=1,\n",
    "        #    #   shuffle=False,\n",
    "        #    #   batch_size = batch,\n",
    "        #    #   verbose=0,\n",
    "        #    #   callbacks = callback)\n",
    "##\n",
    "        #    dic =vae.test_step([tmp])\n",
    "        #    val_tot_loss_ = dic['val_loss']\n",
    "        #    val_reconstr_loss_ = dic['val_reconstruction_loss'] \n",
    "        #    val_kl_loss_ = dic['val_kl_loss']\n",
    "    #\n",
    "    #\n",
    "        #    val_tot_loss += val_tot_loss_\n",
    "        #    val_reconstr_loss += val_reconstr_loss_\n",
    "        #    val_kl_loss += val_kl_loss_\n",
    "        #history['history']['val_total_loss'].append(val_tot_loss/test_iter)\n",
    "        #history['history']['val_reconstruction_loss'].append(val_reconstr_loss/test_iter)\n",
    "        #history['history']['val_kl_loss'].append(val_kl_loss/test_iter)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #histories.append(history)\n",
    "    \n",
    "    \n",
    "    # save file h5\n",
    "    if save:\n",
    "        print(\"\\t\\t---->creating h5 file: \" + savename + \".h5\")\n",
    "        vae.save(savename + \".h5\", save_format='h5')\n",
    "    \n",
    "            #print(history.history)\n",
    "    #validation_data=(data_test,data_test)\n",
    "    # plot model\n",
    "    print(history['history']['total_loss'])\n",
    "    \n",
    "    #plot_history(history, path=savename+',training.png')\n",
    "    \n",
    "    counter = 0\n",
    "    fidelity = 0\n",
    "    # save new files\n",
    "    if savefiles:\n",
    "        for file in wavefuns:\n",
    "            name = f\"{counter}_wavefun__{model.getInfo()}.txt\"\n",
    "            # predict probabilities\n",
    "            tmp = np.array(vae.decoder.predict(np.array([file]))[0])\n",
    "            #print(tmp-file)\n",
    "\n",
    "            # save to file\n",
    "            fil = open(folderSaveNew + name, \"wb\")\n",
    "            np.save(fil, tmp)\n",
    "            fil.close()\n",
    "\n",
    "            fidelity += theirFidelity(tmp, file)\n",
    "            if counter == maximum:\n",
    "                break\n",
    "            counter+=1\n",
    "    else:\n",
    "        for i in np.random.randint(len(wavefuns), size=batch):\n",
    "            file = wavefuns[i]\n",
    "            \n",
    "            # predict probabilities\n",
    "            tmp = np.array(vae.encoder.predict(np.array([file]))[0])\n",
    "            tmp = np.array(vae.decoder.predict([tmp])[0])\n",
    "            #print(file, tmp, \"\\n\\n\\n\")\n",
    "            \n",
    "            # fidelity\n",
    "            fidelity += theirFidelity(tmp, file)\n",
    "            if counter == maximum:\n",
    "                break\n",
    "            counter+=1\n",
    "            \n",
    "    return vae, fidelity/counter#,entropy_before/counter, entropy/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a42081-f2f4-4277-9465-32ba41c377f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518fc553-4649-464c-bfc7-765ac58c0fbd",
   "metadata": {},
   "source": [
    "# LOOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d787c-b4fc-4e33-a1af-b466ca12d092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96c3fcb1-d200-4cef-bc8d-c00129135863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1]),\n",
       " array([   5.        ,  146.06896552,  287.13793103,  428.20689655,\n",
       "         569.27586207,  710.34482759,  851.4137931 ,  992.48275862,\n",
       "        1133.55172414, 1274.62068966, 1415.68965517, 1556.75862069,\n",
       "        1697.82758621, 1838.89655172, 1979.96551724, 2121.03448276,\n",
       "        2262.10344828, 2403.17241379, 2544.24137931, 2685.31034483,\n",
       "        2826.37931034, 2967.44827586, 3108.51724138, 3249.5862069 ,\n",
       "        3390.65517241, 3531.72413793, 3672.79310345, 3813.86206897,\n",
       "        3954.93103448, 4096.        ]),\n",
       " 'D:\\\\Uni\\\\SEMESTERS\\\\PRACE\\\\CONDENSED_GROUP_CLOUD_UNI\\\\Transverse_Ising\\\\Transverse_Ising_ETH\\\\IsignTransverse_ETH\\\\resultsWAVE5\\\\disorder\\\\PBC\\\\')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination = f\"{IsingPath}{kPSep}resultsWAVE5{kPSep}disorder{kPSep}PBC{kPSep}\"\n",
    "modelik = IsingDisorder(12,1,0,0.8,0,0.83,0.1,0)\n",
    "N=modelik.N\n",
    "L = 12\n",
    "latent_dims = np.linspace(5, N, 30)\n",
    "ws = np.array([0.1*i for i in range(1,2)])\n",
    "epo = 10\n",
    "batch = 10\n",
    "filenum = 10\n",
    "layer_num = 2\n",
    "# for pytorch mode\n",
    "parameters = {\"epochs\" : epo, 'batch_size' : batch, 'display_epoch' : 5, 'learning_rate' : 1e-3, 'num_batches':batch}\n",
    "\n",
    "ws, latent_dims, destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b677303-7d99-4317-b0f6-e4cd572d2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43931a1-6cd2-4a2b-9117-4f2c4ab5904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15e199-6eea-431b-8c62-76314d03fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fb808a0-0084-4855-a0f5-ab2ea42d606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 6.00 GiB total capacity; 4.76 GiB already allocated; 0 bytes free; 4.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8824/3127660843.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlat_dim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 modelik2 = Modelik(parameters, folder, lat_dim = int(lat_dim), verbosity=0,\n\u001b[0m\u001b[0;32m     11\u001b[0m                                    \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    \u001b[0mn_qubits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilenum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilenum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8824/2711009892.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parameters, data_path, lat_dim, verbosity, n_layers, n_qubits, trainsize, filenum, load)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Prepare model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         self.vae, self.train_loaders, self.test_loaders, self.optimizer = self.prepare_model(\n\u001b[0m\u001b[0;32m     37\u001b[0m             load=load)\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8824/2711009892.py\u001b[0m in \u001b[0;36mprepare_model\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mVAE_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         vae = VariationalAutoencoder(VAE_layers.get('encoder'), VAE_layers.get(\n\u001b[0m\u001b[0;32m     66\u001b[0m             'decoder'), VAE_layers.get('logvar'), VAE_layers.get('mu')).double().to(self.device)\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\MyAnacondaDont\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mD:\\Programy\\MyAnacondaDont\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\MyAnacondaDont\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\MyAnacondaDont\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\MyAnacondaDont\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 6.00 GiB total capacity; 4.76 GiB already allocated; 0 bytes free; 4.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer_num in range(3,4):\n",
    "    with open(destination + f\"heatmap_layernum={layer_num},filenum={filenum},L={L},epo={epo},batch={batch}.txt\", \"w\") as mapka:\n",
    "        for w in ws:\n",
    "            modelik = IsingDisorder(L,1,0,0.8,0,0.83,w,0)\n",
    "            N=modelik.N\n",
    "            folderLog = destination + \"_\" + modelik.getInfo() + kPSep\n",
    "            folder = folderLog + \"wavefunctions\" + kPSep\n",
    "            \n",
    "            for lat_dim in latent_dims:\n",
    "                modelik2 = Modelik(parameters, folder, lat_dim = int(lat_dim), verbosity=0,\n",
    "                                   n_layers=layer_num,\n",
    "                                   n_qubits=L,filenum = filenum,\n",
    "                                   load=None)\n",
    "                #enc,fidelity = fileAutoencode(destination, modelik, int(lat_dim), epo,\n",
    "                #                              layer_num=layer_num,\n",
    "                #                              filenum=filenum,batch=batch,\n",
    "                #                              verbose = 0,\n",
    "                #                              save = False, savefiles = False)\n",
    "                fid = modelik2.fidelity\n",
    "                justPrinter(mapka, \"\\t\", [f\"{w:.2f}\", f\"{int(lat_dim) / float(N):.5f}\"\n",
    "                                          ,f\"{fid:.7f}\"], width = 10)\n",
    "                mapka.flush()\n",
    "\n",
    "#kullback_leibler_divergence\n",
    "#mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b88c21-5c27-4fc6-a8ce-a855d81768c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2251a-5b71-4c10-80ab-035db2bcdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims([[1,2],[3,4]], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dc623-ee3c-45c3-a792-8106030a50be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aaf48-5f29-4659-91eb-8957f0b07fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b15a38-4f9b-4018-9e11-768e279316dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b40a3-23fb-4e24-ab3e-dea5fea9cc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89e925-c935-4afb-a154-785c5ae72eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
