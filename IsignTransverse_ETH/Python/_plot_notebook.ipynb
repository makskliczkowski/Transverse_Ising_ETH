{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23411ad-4bdf-4f97-9b3e-0df42a06c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import imageio\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from scipy.optimize import curve_fit\n",
    "from joblib import Parallel, delayed\n",
    "#plt.style.use(['science','ieee','no-latex'])\n",
    "\n",
    "# get all the colors\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39babdcd-ad96-450b-abe0-41190ca85bf0",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2744a9bb-1130-4b0f-a9d2-8f85c0b3dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rafal.swietek/.matplotlib\n"
     ]
    }
   ],
   "source": [
    "boundary_conditions = {0 : \"PBC\", 1:\"OBC\"}\n",
    "print(matplotlib.get_configdir())\n",
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c3e1ad-c589-4928-a43e-d651b50741b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingDisorder:\n",
    "    N = 1\n",
    "    M = 1\n",
    "    def __init__(self, L, J, J0, g, g0, h, w, _BC):\n",
    "        self.L = L\n",
    "        self.J = J\n",
    "        self.J0 = J0\n",
    "        self.g=g\n",
    "        self.g0=g0\n",
    "        self.h=h\n",
    "        self.w =w\n",
    "        self.BC=_BC\n",
    "        self.directory = \"results\" + kPSep\n",
    "        self.N = math.pow(2,L)\n",
    "        \n",
    "    def getInfo(self):\n",
    "        return f'L={self.L},J0={self.J0:.2f},g={self.g:.2f},g0={self.g0:.2f},h={self.h:.2f},w={self.w:.2f}' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2dfa0a-8338-4b90-b662-c749f3a82a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kPSep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000004?line=0'>1</a>\u001b[0m a\u001b[39m=\u001b[39mIsingDisorder(\u001b[39m12\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0.8\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0.8\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000004?line=1'>2</a>\u001b[0m a\u001b[39m.\u001b[39mgetInfo()\n",
      "\u001b[1;32m/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb Cell 4'\u001b[0m in \u001b[0;36mIsingDisorder.__init__\u001b[0;34m(self, L, J, J0, g, g0, h, w, _BC)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000003?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m=\u001b[39mw\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBC\u001b[39m=\u001b[39m_BC\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000003?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirectory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m kPSep\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rafal.swietek/Projects/Transverse_Ising_ETH/IsignTransverse_ETH/Python/_plot_notebook.ipynb#ch0000003?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m,L)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kPSep' is not defined"
     ]
    }
   ],
   "source": [
    "a=IsingDisorder(12,1,0,0.8,0,0.8,0.1,0)\n",
    "a.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c3491-b676-411a-9a5f-492d0eb23c22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c6b91-6c88-4724-9267-56391f765f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = itertools.cycle(['o','s','v', '+'])\n",
    "colors_ls = list(mcolors.TABLEAU_COLORS)[:30]\n",
    "colors = itertools.cycle(sns.color_palette()[:3])\n",
    "TWOPI = math.pi * 2\n",
    "kPSep = os.sep\n",
    "\n",
    "disorder_pbc = f\"..{kPSep}results{kPSep}disorder{kPSep}PBC{kPSep}\"\n",
    "symm_pbc = f\"C:{kPSep}Users{kPSep}maxgr{kPSep}Desktop{kPSep}Wyniken{kPSep}results2_2{kPSep}symmetries{kPSep}PBC{kPSep}\"\n",
    "IsingPath = f\"D:{kPSep}Uni{kPSep}SEMESTERS{kPSep}PRACE{kPSep}CONDENSED_GROUP_CLOUD_UNI{kPSep}Transverse_Ising{kPSep}Transverse_Ising_ETH{kPSep}\"+\\\n",
    "    f\"IsignTransverse_ETH\"\n",
    "IsingPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bdb78-e53e-47fc-b7ca-757b769b5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=1.15\n",
    "hstep = 0.02\n",
    "\n",
    "hvalues = [h+i*hstep for i in range(3)]#+[1.4,1.42,1.45,1.47,1.5,1.55,1.57]\n",
    "# perturbations\n",
    "pert_vec = np.linspace(1e-3, 1e-2, 11);\n",
    "pert_vec = np.append(pert_vec,np.linspace(1e-2, 3.9e-1, 20));\n",
    "pert_vec,hvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720a31a-ef2a-4ab7-8fe4-063f9895afa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------\n",
    "# String separators and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7a74b-8340-450c-b36d-5e3b0d5f8cb5",
   "metadata": {},
   "source": [
    "### Concatenate list to a string given a separator\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0cab0-af8c-44ea-94f6-18d0c85a69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert  \n",
    "def listToString(s, separator = \",\"): \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"   \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        if isinstance(ele, (float,int)):\n",
    "            str1 += \"{:.3f}\".format(ele) +separator \n",
    "        else:\n",
    "            str1 += str(ele) +separator  \n",
    "    \n",
    "    # return string  \n",
    "    return str1[:-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf76e4f-541b-46ed-a946-c82876350bb5",
   "metadata": {},
   "source": [
    "### Creating a folder given a directory\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bc86e-7e1f-44cd-b1cf-ba3458769ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(directories, silent = False):\n",
    "    for folder in directories:\n",
    "        try:\n",
    "            if not os.path.isdir(folder):\n",
    "                os.makedirs(folder)\n",
    "                if not silent:\n",
    "                    print(\"Created a directory : \", folder)\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" % folder)      \n",
    "# Guard against race condition\n",
    "        except OSError as exc: \n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3d2a2-7b23-4dfb-9e3c-a4e1d5303909",
   "metadata": {},
   "source": [
    "### Reading random number from a folder given a condition\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20937d58-6772-48ca-b22e-948426d795c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = lambda x : x.endswith('.dat')\n",
    "def readRandomFile(folder, cond, withoutFolder = False):\n",
    "    choice = random.choice(os.listdir(folder))\n",
    "    #print(choice)\n",
    "    maxlen = len(os.listdir(folder))\n",
    "    counter = 0\n",
    "    while not cond(choice):\n",
    "        choice = random.choice(os.listdir(folder))\n",
    "        if counter > maxlen:\n",
    "            raise\n",
    "        counter += 1\n",
    "    if withoutFolder:\n",
    "        return choice\n",
    "    else:\n",
    "        return folder + choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d1e99-bbdb-4c6d-98a2-0c01a3ad5b24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### finding things in the list that are the same not to print them twice\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990cf6f-49cc-4c45-9ec7-71af692d004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDifferentElementsStrings(listOfStrings):\n",
    "    different = []\n",
    "    the_same = []\n",
    "    if len(listOfStrings) != 0:\n",
    "        # create the intersection to distinguish same elements\n",
    "        same = set(listOfStrings[0].split(\",\"))\n",
    "        #print(same)\n",
    "        for i in range(1,len(listOfStrings)):\n",
    "            tmp = set(listOfStrings[i].split(\",\"))\n",
    "            same = same.intersection(tmp)\n",
    "            #print(same)\n",
    "        # after having the intersection find the list of names to put into legend that are different\n",
    "        for i in range(0, len(listOfStrings)):\n",
    "            tmp = set(listOfStrings[i].split(\",\")) - same\n",
    "            different.append(listToString(tmp))\n",
    "        the_same = list(same)\n",
    "    # return same and list of different strings in the form of tuple\n",
    "    return (listToString(np.sort(the_same)), different)\n",
    "\n",
    "findDifferentElementsStrings([\"g=1,h=2,w=3,a=1,pert=0.2\", \"g=1,h=3,w=4,a=1,pert=0.2\",\"g=1,h=2,w=4,a=2,pert=0.2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b894d-1935-429e-8289-07afd9217efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print to file with adjusting column width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4c9ca-a34c-4186-9e2a-d90044b8f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def justPrinter(file,sep=\"\\t\", elements =[], width = 8, endline = True):\n",
    "    for item in elements:\n",
    "        file.write((str(item) + sep).ljust(width))\n",
    "    if endline:\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570338d-fe98-442d-b58e-b60531c9016c",
   "metadata": {},
   "source": [
    "#### return list of axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a582ec-be77-4cea-8ac8-64b49bea60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfAxis(num, figsize = (10,10), dpi = 100):\n",
    "    fig, ax = plt.subplots(num, figsize=(10,10), dpi = 100)\n",
    "    sns.set_style(\"ticks\")\n",
    "    # set axis for it to always be a list\n",
    "    axis = []\n",
    "    if num > 1: # if we have many columns to plot\n",
    "        axis = [ax[i] for i in range(num)]\n",
    "    else:\n",
    "        axis = [ax]\n",
    "    return fig, axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913347f2-2a41-4299-a344-80cc9949d5bc",
   "metadata": {},
   "source": [
    "------------\n",
    "## IMAGES AND ETC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f6f03-acf4-423c-a98d-43fb41b77f94",
   "metadata": {},
   "source": [
    "#### make gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18b377-3e3a-4511-906b-fc69f05eb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makegif(directory, name, distinguishers, images,fps=1.0):\n",
    "    folder = f'{directory}plots{kPSep}' \n",
    "    kwargs_write = {'fps':fps, 'quantizer':'nq'}\n",
    "    imageio.mimsave(folder + f'{name}_{listToString(distinguishers)}.gif', images, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f1a6c-83dc-49a2-9fe9-cba73bbaeb96",
   "metadata": {},
   "source": [
    "#### make axis style for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8064bd7-9dfb-4937-b278-77dae17a5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPlotElems(ax, xlabel, ylabel, xscale, yscale, legend = True, leg_names = [], font_size = 8.5, xlim =[], ylim=[]):\n",
    "    ax.set(xlabel = xlabel, ylabel = ylabel)\n",
    "    ax.set_yscale(yscale)\n",
    "    ax.set_xscale(xscale)\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(leg_names\n",
    "                , frameon=False\n",
    "                , loc='best'\n",
    "                , fontsize=font_size)\n",
    "    \n",
    "    xmin, xmax = xlim\n",
    "    ymin, ymax = ylim\n",
    "    if xmin != None and xmax != None:\n",
    "        ax.set_xlim([xmin,xmax])\n",
    "    if ymin != None and ymax != None:\n",
    "        ax.set_ylim([ymin, ymax])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bc0e9-c85e-4058-a830-e1d4daab26f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "-------------------------\n",
    "# Handle all the files starting from a given name given by the user and distinguished by the parameters and return the list of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970419d8-0cdb-4d22-98d7-48b9b02b693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_header(filename):\n",
    "    with open(filename) as f:\n",
    "        first = f.read(1)\n",
    "        return first not in '.-0123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6876c-4a39-4b96-8e1a-f85b1ab40a0a",
   "metadata": {},
   "source": [
    "### Fit functions\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a2ffc-a762-4399-88b3-c320210e8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_fit(x, lambd):\n",
    "    return lambd * np.exp(-lambd * np.abs(x))\n",
    "    \n",
    "def laplace_fit(x, lambd,mu):\n",
    "    return np.exp(-np.abs(x - mu)/lambd)/(2*lambd)\n",
    "\n",
    "def exponential_fit(x, lambd,sigma):\n",
    "    return sigma * np.exp(-lambd * np.abs(x))\n",
    "\n",
    "def gauss_fit(x, mu, sigma):\n",
    "    return np.exp(-0.5*np.power(x-mu,2)/(sigma*sigma))/(sigma*math.sqrt(2*np.pi))\n",
    "\n",
    "def linear_fit(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "fitDic = {\n",
    "    poisson_fit : ('poisson','$\\lambda$*exp(-$\\lambda$x):' ,'$\\lambda$='),\n",
    "    laplace_fit : ('laplace','exp(-|x-$\\mu$|/$\\lambda$)/2$\\lambda$:',\"$\\mu$=\",'$\\lambda$='),\n",
    "    exponential_fit : ('exponential','$\\sigma$*exp(-$\\lambda$x):', '$\\sigma$=', '$\\lambda$=' ),\n",
    "    gauss_fit : ('gaussian','exp(-$(x-\\mu)^2$/2($\\sigma^2$))/($\\sigma$*$\\sqrt{2\\pi}$):', '$\\mu$=', '$\\sigma$=' ),\n",
    "    linear_fit : ('linear', 'a*x+b:', 'a=', 'b=')\n",
    "}\n",
    "fitDic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350cfd4-3184-42e1-92df-fc43538e6632",
   "metadata": {},
   "source": [
    "##### distinguish between different fits and append it to the legend list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569a104-f73e-4105-94e8-cc037ad81c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFitFunction(fitFun, appendList, popt, chi_p_value):\n",
    "    tmp = fitDic[fitFun]\n",
    "    fitname = tmp[0] + ',' + tmp[1]\n",
    "    counter = 0\n",
    "    for param in tmp[2:]:\n",
    "        appendList.append(f'{param}' + \"{:.3f}\".format(popt[counter]))\n",
    "        counter += 1\n",
    "    appendList.append(f'\\nChi={chi_p_value:.3f}')\n",
    "    return fitname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74f51a-9d98-41da-bba1-2e6911380e66",
   "metadata": {},
   "source": [
    "##### fit to dataframe from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04474c6d-2a68-4f27-8206-20c7214a7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitToDataframe(fitFun, x, y):\n",
    "    # fit curve\n",
    "    xdata = np.array(x)\n",
    "    ydata = np.array(y)\n",
    "    popt, pcov = curve_fit(fitFun, x, ydata)\n",
    "    predict = np.array(fitFun(xdata, *popt))\n",
    "    \n",
    "    ydataChi = ydata / np.sum(ydata)\n",
    "    popt2, pcov2 = curve_fit(fitFun, x, ydataChi)\n",
    "    predictChi = np.array(fitFun(xdata, *popt2))\n",
    "    predictChi = predictChi / np.sum(predictChi)\n",
    "    \n",
    "    #print(np.sum(ydataChi),np.sum(predictChi))\n",
    "    chi, pval_chi = stats.chisquare(f_obs=ydataChi, f_exp=predictChi)\n",
    "    return popt, chi, pd.DataFrame(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe69532-ee46-4ea8-9358-733c3d524d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------\n",
    "### Give dataframes from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a74c5-420d-46ad-8b57-8abaa808dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_all_files(directory, file_begin, columns, distinguishers, separator = \"\\t\\t\", fitfunctions = []):\n",
    "    # columns is a list of tuples of column number and its name [(0:...)] etc -> 0 is an idx\n",
    "    dfs = []\n",
    "    # make fit before\n",
    "       \n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename[-4:] != \".dat\" or not filename.startswith(file_begin):\n",
    "            continue\n",
    "        # split the name and the parameters\n",
    "        splitter = filename[:-4].split(\"_\")\n",
    "        # split each of the parameters \n",
    "        params = splitter[-1].split(\",\")\n",
    "        skip = False\n",
    "        \n",
    "        for element in distinguishers:\n",
    "            in_bucket = False\n",
    "            # check elements in each element_bucket\n",
    "            for bucket in element:\n",
    "                if bucket in params:\n",
    "                    in_bucket = True\n",
    "            # if none of possible parameters from individual bucket is in params => skip\n",
    "            if not in_bucket:\n",
    "                skip = True\n",
    "                break\n",
    "                \n",
    "        if not skip or len(distinguishers) == 0:\n",
    "            skip_rows = 0\n",
    "            # check if there is a header present already\n",
    "            if check_header(directory + filename):\n",
    "                skip_rows = 1\n",
    "            tmp = pd.read_csv(directory + filename, sep = separator, header=None, skiprows = skip_rows, index_col = 0)\n",
    "\n",
    "            # check if nan's are in and skip that column if all are nans\n",
    "            tmp = tmp.loc[:,tmp.notna().all(axis=0)]\n",
    "            # if there is still something to read then read!\n",
    "            if len(columns) > 0:\n",
    "                names = [column[0] for column in columns]\n",
    "                numbers = [column[1] for column in columns]\n",
    "                \n",
    "                # take everything except the index\n",
    "                tmp = tmp[numbers[1:]]\n",
    "                tmp.columns = names[1:]\n",
    "            # append new df to a list of dataframes with its params   \n",
    "            dfs.append((tmp, listToString(params)))\n",
    "            for fitFun in fitfunctions:\n",
    "                # always take the first column for the fit\n",
    "                popt,chi, fit_df = fitToDataframe(fitFun, tmp.index, tmp[columns[1][0]])\n",
    "                fit_df.index = tmp.index\n",
    "                # add labels\n",
    "                params_fit = []\n",
    "                fitname = addFitFunction(fitFun, params_fit,popt, chi)\n",
    "                name,function  = fitname.split(',')\n",
    "                params_fit = [function] + params_fit + [name]\n",
    "                #print(params_fit)\n",
    "                # append to dataframes\n",
    "                dfs.append((fit_df, f'{listToString(params_fit,separator=\":\")},{listToString(params,separator=\",\")}'))    \n",
    "    #print(dfs)\n",
    "    return dfs\n",
    "\n",
    "tmp = handle_all_files(symm_pbc, \"perturbationOperatorsDist\", columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)]\\\n",
    "                       , distinguishers = [[\"h=1.35\"],[\"g=0.80\"],['pert=0.0010']], fitfunctions = [gauss_fit])\n",
    "#tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f543ad-4772-4d61-8b0c-6c6341a2a35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42d99a-50d6-46b5-ae5b-2a87adb9f321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21551e63-89fc-482f-8bae-18abc6aff5bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Operators plotter as a function of energies, plots all elements connecting files with parameters present in the distinguisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598e364-7c54-4a95-928e-c28781deae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_av_operator_dfs(directory, file_begin, columns, distinguishers, operator_name,\\\n",
    "                         scale_x = \"linear\", scale_y = \"linear\",chi_squared = False,\\\n",
    "                         separator = \"\\t\\t\", fitfunctions = [],gif = False,save_files=True,\\\n",
    "                         xmin = -0.05, xmax = 0.05, ymin = 0.01, ymax = 100):\n",
    "    # clean plot    \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    \n",
    "    fitter = []\n",
    "    for fitFun in fitfunctions:\n",
    "        fitter.append(fitDic[fitFun][0])\n",
    "        \n",
    "    # save in folder\n",
    "    folder = f'{directory}plots{kPSep}' \n",
    "    createFolder([folder])\n",
    "    #print(directory)\n",
    "    # find the list of dataframes\n",
    "    a = handle_all_files(directory, file_begin, columns, distinguishers, separator = separator, fitfunctions = fitfunctions)\n",
    "    if len(a) == 0:\n",
    "        return\n",
    "    else:\n",
    "        #unpack\n",
    "        listOfDf = [item[0] for item in a]\n",
    "        labels = ([item[1] for item in a])\n",
    "        \n",
    "    #print(labels)\n",
    "    # create number of figures to be plotted\n",
    "    fig, axis = listOfAxis(len(columns) - 1, figsize=(10,10), dpi = 100)\n",
    "\n",
    "    # for files\n",
    "    same_and_diff = findDifferentElementsStrings(labels)\n",
    "    savefile = file_begin + \"_\" + same_and_diff[0]\n",
    "    \n",
    "    # plot\n",
    "    colorki = [next(colors) for i in range(len(listOfDf))]\n",
    "    markerki = [next(markers) for i in range(len(listOfDf))]\n",
    "    \n",
    "    # iterate over dataframes\n",
    "    df_iter = 0\n",
    "    for df in listOfDf:\n",
    "        itr = 0\n",
    "        for col in df.columns:\n",
    "            df[col].plot(ax = axis[itr]\n",
    "                    , linewidth=0.5\n",
    "                    , marker=markerki[df_iter]\n",
    "                    , markersize=1.8\n",
    "                    , color = colorki[df_iter])\n",
    "            itr+=1\n",
    "        df_iter += 1\n",
    "    #print(\"same_and_diff is:\",np.sort(same_and_diff[1]))\n",
    "    # set the style\n",
    "    for i in range(len(columns) - 1):\n",
    "        \n",
    "        setPlotElems(axis[i], columns[0][0],\n",
    "                     columns[i+1][0], xscale = scale_x, yscale=scale_y,\n",
    "                     legend = len(listOfDf) > 1, \n",
    "                     leg_names = same_and_diff[1],\n",
    "                     font_size = 8.5,\n",
    "                     xlim =[xmin,xmax], ylim=[ymin,ymax])\n",
    "\n",
    "    \n",
    "    # add title\n",
    "    fig.suptitle(operator_name + \" for \" + same_and_diff[0], y=0.91)\n",
    "    # set fitfunctions\n",
    "    # GIF\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    if gif:\n",
    "        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close()\n",
    "    if save_files:\n",
    "        fits = \".png\"\n",
    "        if(len(fitfunctions) > 0):\n",
    "            fits = f',{listToString(fitter,\",\")}{fits}'\n",
    "            #print(fits)\n",
    "        plt.savefig(f'{folder + savefile},scale_y={scale_y},scale_x={scale_x }{fits}')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "            \n",
    "    #\n",
    "    return image\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6d72c-2d30-4fc1-bf95-6dede772f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_x                    \n",
    "#plot_av_operator_dfs(symm_pbc, \"SigmaX\",columns = [(\"E/L\",0), (\"<n|$S_x$|n>\",1)], distinguishers = [], operator_name = \"<n|$S_x$|n>\") \n",
    "# sigma_x prob dist\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSigmaX\",columns = [(\"$S^x_{nn}$\", 0), (\"P($S^x_{nn}$)\",1)], distinguishers = [[\"L=18\"],[\"g=0.80\"], [\"h=1.00\",\"h=1.50\"]],\n",
    "#                      operator_name = \"Probability distribution of $S^x_{nn}$\", scale_y = 'log', fitfunctions = []) \n",
    "# sigma x repulsion\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSpecRapSigmaX\",columns = [(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)],\n",
    " #distinguishers = [[\"L=19\",\"L=18\", \"L=17\", \"L=16\"],[\"g=0.8\"], [\"h=1.00\"]], operator_name = \"(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)\", scale_y = 'linear', separator = '\\t')\n",
    "# sigma x prob dist all sectors\n",
    "#plot_av_operator_dfs(symm_pbc, \"ProbDistSpecRapSigmaXAllSectors\",columns = [(\"$S^x_{nn} - $S^x_{n-1,n-1}$\", 0), (\"P(r)\",1)],\n",
    " #distinguishers = [], operator_name = \"P(r) for all sectors\", scale_y = 'linear', separator = '\\t\\t') \n",
    "\n",
    "# energy difference\n",
    "#plot_av_operator_dfs(symm_pbc + \"/EnergyDiff/\", \"perturbationEnergyDiffDist\",columns = [(\"E-E'\", 0), (\"P(E-E')\",1)], \n",
    "#                     distinguishers = [[\"L=18\"],[\"g=0.80\"], [\"h=1.80\"], [\"pert=0.0100\", \"pert=0.0300\", \"pert=0.1100\", \"pert=0.1500\"]],\n",
    "#                     operator_name = \"P(E-E')\", scale_y = 'log', fitfunctions =[], xmin = -0.25, xmax = 0.25) \n",
    "# operators dist\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c543b-9fca-4021-ab68-245f4c2a06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hval in hvalues:    \n",
    "    images = []\n",
    "    for i in pert_vec:\n",
    "        #print(i)\n",
    "        #im = plot_av_operator_dfs(symm_pbc, \"perturbationOperatorsDist\",columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)],\n",
    "                        #distinguishers = [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"], [f\"pert={i:.4f}\"]],\n",
    "                        #operator_name = \"Probability distribution with perturbation\", scale_y =\"log\",\\\n",
    "                         #fitfunctions=[laplace_fit, gauss_fit],gif=True,save_files = False,\\\n",
    "                         #xmin = -0.1, xmax = 0.1, ymin = 0.1, ymax = 500) \n",
    "        #images.append(im)     \n",
    "    #makegif(symm_pbc, \"perturbationOperatorsDist$S^x$\", [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"]], images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121b81b-2c01-4cf3-b16e-c3717d365438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in pert_vec[5:17]:\n",
    "    print(i)\n",
    "    images = []\n",
    "    for hval in hvalues:    \n",
    "        \n",
    "        print(hval)\n",
    "        #im = plot_av_operator_dfs(symm_pbc, \"perturbationOperatorsDist\",columns = [(\"$S^x_p$ - $S^x_{p+dp}$\", 0), (\"$P(S^x_p-S^x_{p+dp})$\", 1)],\n",
    "                        distinguishers = [[\"L=18\"],[\"g=0.80\"], [f\"h={hval:.2f}\"], [f\"pert={i:.4f}\"]],\n",
    "                        operator_name = \"Probability distribution with perturbation\", scale_y =\"log\",\\\n",
    "                         fitfunctions=[laplace_fit, gauss_fit],gif=True,save_files = False,\\\n",
    "                         xmin = -0.1, xmax = 0.1, ymin = 0.1, ymax = 500)\n",
    "        if(len(im)>0):\n",
    "            images.append(im)\n",
    "        \n",
    "    makegif(symm_pbc, \"perturbationOperatorsDist$S^x$\", [[\"L=18\"],[\"g=0.80\"], [f\"pert={i:.4f}\"]], images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ipr scaling with h\n",
    "#plot_av_operator_dfs(symm_pbc, \"IprScaling\",columns = [(\"h\", 0), (\"ipr\", 1), (\"Information entropy\", 2), (\"$r_{goe}$\",3)], distinguishers = [[\"L=18\"],[\"g=1.20\",\"g=0.80\", \"g=0.40\"]], operator_name = \"System scaling with h\")\n",
    "# moments of probability sigma_x fluct\n",
    "#plot_av_operator_dfs(symm_pbc, \"Moments\",columns = [(\"h\", 0), (\"$U_B$\", 1), (\"Kurtossis\", 2), (\"$\\sigma$\",3)], distinguishers = [[\"L=18\"],[\"g=1.20\",\"g=0.80\"]], operator_name = \"System scaling with h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd041cf-f1a3-438a-aea7-a047b120ed1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca04311-6f9e-4eba-b9f5-c444d26bef4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot only distributions in different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08422308-a8a9-4bea-8af9-638848300620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(directory, file_begin, columns, distinguishers, distribution_name, column_num = [], separator = \"\\t\\t\"):\n",
    "    try:\n",
    "        folder = 'plots/' + directory \n",
    "        if not os.path.isdir(folder):\n",
    "            os.makedirs(folder)\n",
    "    except OSError:\n",
    "        print(\"Creation of the directory %s failed\" % (folder))\n",
    "    else:\n",
    "        sns.set_style(\"ticks\")\n",
    "        # find the list of dataframes\n",
    "        listOfDf = handle_all_files(directory, file_begin,columns, distinguishers, column_num, separator)\n",
    "        labels = [item[1] for item in listOfDf]\n",
    "        same_and_diff = findDifferentElementsStrings(labels)\n",
    "        \n",
    "        # plot \n",
    "        counter = 0\n",
    "        for df in listOfDf:\n",
    "            savefile = file_begin + \"_\" + same_and_diff[0] + \",\" + labels[counter]\n",
    "            fig, ax = plt.subplots(len(columns) - 1, figsize=(12,8), dpi = 120) \n",
    "            axis = []\n",
    "            if len(columns) > 2: # if we have many columns to plot\n",
    "                axis = [ax[i] for i in range(len(columns) - 1)]\n",
    "            else:\n",
    "                axis = [ax]\n",
    "            \n",
    "            step = 0.01\n",
    "            _min = np.min(df[0].index)\n",
    "            _max = np.max(df[0].index)\n",
    "            new_ticks = [_min + step * i for i in range(0, int((_max-_min)/step) + 1)]\n",
    "            print(new_ticks)\n",
    "            itr = 0\n",
    "            for col in df[0].columns:      \n",
    "\n",
    "                df[0][col].plot.bar(ax = axis[itr], rot=15, color = next(colors))\n",
    "                axis[itr].set(title = distribution_name + \" for \" + same_and_diff[0] + \",\" + same_and_diff[1][counter]\n",
    "                            ,xlabel = col\n",
    "                            ,ylabel = distribution_name)\n",
    "                #axis[itr].get_legend().remove()\n",
    "                #(same_and_diff\n",
    "                #            , frameon=False\n",
    "                #            , loc='best'\n",
    "                #            , fontsize=8)\n",
    "                axis[itr].set_xticks(np.interp(new_ticks, df[0].index, np.arange(df[0].size)))\n",
    "                axis[itr].set_xticklabels(new_ticks)\n",
    "                itr+=1\n",
    "            counter+=1\n",
    "            plt.savefig(folder + savefile + \".pdf\")\n",
    "            plt.savefig(folder + savefile + \".png\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#plot_distribution(symm_pbc, \"ProbDistSpecRapSigmaX\", columns = [\"P(|r|)\", \"<n + 1|$S_x$|n + 1> - <n|$S_x$|n>\"], distinguishers = [], distribution_name= \"P(|r|)\", separator = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e3eea-5a41-4a33-87fb-ae7c493e828f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PLOT HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c566d-6ddf-4ca5-aca6-0757a23e222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_map(destination, filename, columns, separator = \"\\t\"):\n",
    "    # save in folder\n",
    "    df = pd.read_csv(f\"{destination}{filename}\", sep = separator, header=None)\n",
    "    df = df.dropna(axis = 1)\n",
    "    #mapka\n",
    "    df.columns = columns\n",
    "    #return df    \n",
    "    #df.set_index([columns[0], columns[1]], inplace = True)\n",
    "    #cols = df.columns\n",
    "    #df[cols[0]] = df[cols[0]].replace('  nan',0)\n",
    "    \n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,10), dpi = 100)\n",
    "    savefile = f\"{destination}{filename[0:-4]}.pdf\"\n",
    "\n",
    "    #print(cols)\n",
    "\n",
    "    #pivotted = df.pivot(cols[0])\n",
    "    # PLOT \n",
    "    z = np.array(df.loc[:,columns[2]])\n",
    "    z = np.array([0 if i == '  nan' else float(i) for i in z])\n",
    "    #print(z)\n",
    "    x = np.array(df.loc[:,columns[0]])\n",
    "    x = np.unique(x)\n",
    "    print(x)\n",
    "    y = np.array(df.loc[:,columns[1]])\n",
    "    y = np.unique(y)\n",
    "    print(y)\n",
    "\n",
    "    X,Y = np.meshgrid(y,x)\n",
    "\n",
    "    Z=z.reshape(len(x),len(y))\n",
    "\n",
    "    heatmap = plt.pcolormesh(X,Y,Z,cmap='coolwarm',vmin=np.min(z), vmax=1 )\n",
    "    #pivotted = df.pivot(x,y,z)\n",
    "    #print(pivotted)\n",
    "    #\n",
    "    #mini = np.min(z)\n",
    "    #maxi = np.max(z)\n",
    "    ##print(mini,maxi)\n",
    "    #sns.heatmap(X,Y,Z,cmap='coolwarm',vmin=np.min(z), vmax=np.max(z))\n",
    "    ##sns.heatmap(pivotted,cmap='coolwarm',vmin=np.min(z), vmax=np.max(z))\n",
    "    if(len(columns) == 3):\n",
    "        plt.xlabel(columns[1])\n",
    "        plt.ylabel(columns[0])\n",
    "        \n",
    "    ##ax.set_zscale([0.35,0.535])\n",
    "    plt.title(f\"{columns[2]} for different lattent dimensions and transverse field disorder strength\\n\\\n",
    "    $δg$ in (g+δg) * $S_x$\")\n",
    "    #plt.legend()\n",
    "    fig.colorbar(heatmap)\n",
    "    plt.savefig(savefile)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8192e-06a6-4aa9-8955-03d3fc3885fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = f\"C:{kPSep}Users{kPSep}maxgr{kPSep}Desktop{kPSep}Wyniken{kPSep}WAVES{kPSep}\"\n",
    "plot_heat_map(destination, \"heatmap_layernum=2.txt\", [\"δg\",\"compression C=lat_dim / $2^L$\", \"fidelity (bhattacharya coefficient)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89b0c4-3fee-4a40-b144-f99f110236b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73547c54-f0b3-4201-8229-ce866a6aad5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25943d8c-990d-41ac-8594-f3c46ec46639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa39582-2089-4549-9651-3f6692631880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6645a81-0275-4e69-b6ad-d2a617f2f65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae0cec-3e53-4d45-980e-9d279dac0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d37cb-be13-4aab-aa22-6c7bf45e5953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278057ef-b759-423f-8c12-5e018638c1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535167d-c40e-403d-b3c7-a738e1512c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd16f82-0245-49f3-959e-86170417b833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df36e6-63aa-46a3-9e5f-75fb447b8f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604d88f-957d-400a-8bb4-4a05d7a88033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d7ab51-a9e6-49ed-b446-057f4f38d6fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc8e1b-a96a-457c-9e2e-f017c0357da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationEntropy(stateA, statesB, N, sqrt = False):\n",
    "    ent = 0\n",
    "    for stateB in statesB:\n",
    "        c_k = 0\n",
    "        if not sqrt:\n",
    "            c_k = np.dot(stateA, (stateB))\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                c_k += math.sqrt(stateA[i]) *math.sqrt(stateB[i])\n",
    "                \n",
    "        val = abs(c_k)\n",
    "        ent += val * math.log(val)\n",
    "    return -ent / math.log(0.48 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54959608-1c84-46c5-b789-269de2f5442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theirFidelity(probaA, probaB):\n",
    "    # first state\n",
    "    mini, maxi = np.min(probaA), np.max(probaA)\n",
    "    #print(mini,maxi)\n",
    "    h =2* (np.quantile(probaA,0.75) - np.quantile(probaA,0.25)) * math.pow(len(probaA),-1/2)\n",
    "    bins = [mini + i * h for i in range(int((maxi - mini)/h))]\n",
    "    #print(bins)\n",
    "    \n",
    "    f1, b1 = np.histogram(probaA, density=True, bins=bins)\n",
    "    #plt.hist(probaA, density=True, bins=b1)\n",
    "    # second state\n",
    "    f2, b2= np.histogram(probaB, density = True, bins = b1)\n",
    "    #plt.hist(probaB, density=True, bins=b1)\n",
    "    \n",
    "    f1/=f1.sum()\n",
    "    f2/=f2.sum()\n",
    "    return np.sum(np.sqrt(np.multiply(f1, f2)))\n",
    "\n",
    "probaA = np.array([0.3602150537634409, 0.42028985507246375, \n",
    "  0.373117033603708, 0.36813186813186816, 0.32517482517482516, \n",
    "  0.4175257731958763, 0.41025641025641024, 0.39408866995073893, \n",
    "  0.4143222506393862, 0.34, 0.391025641025641, 0.3130841121495327, \n",
    "  0.35398230088495575])\n",
    "probaA /= np.sum(probaA)\n",
    "\n",
    "probaB = np.array([0.3602150537634409, 0.42028985507246375, \n",
    "  0.373117033603708,\n",
    "  0.463, 0.41025641025641024, 0.39408866995073893, \n",
    "  0.406393862, 0.34, 0.391025641025641, 0.3130841121495327, \n",
    "  0.35398230088495575])\n",
    "probaB /= np.sum(probaB)\n",
    "\n",
    "\n",
    "theirFidelity(np.array(probaA), np.array(probaB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37594429-39c0-4b9e-be57-7574d83a3510",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250827d2-568f-46a8-8795-53550ac5d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n",
    "from keras.callbacks import History\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "from keras import Input, Model\n",
    "from keras import regularizers\n",
    "from keras import initializers, optimizers\n",
    "from plot_keras_history import plot_history\n",
    "from tqdm.keras import TqdmCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, Rescaling\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba52a2-a8c6-41fe-b8ae-f4fd64b3b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347ef2d-277d-4aef-8109-5e5988ed61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e452f84-e6a2-43f6-b464-b395ad01403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f3883-23cc-4015-9279-475b0cee0bba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AUTOENCODER KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73370b5e-3551-48b1-a9ca-44c7199fb92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca4002-4444-4e4a-81a1-8db0ccfc0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e955c3a-1ede-4e17-86de-b2ab3a5e5a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60a9e5-9fc7-47ca-b726-d1be3892de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoEnc(latent_dim, shape, nlayers = 1, alfa = 0.1): \n",
    "    print(f\"creating {nlayers} with input shape {shape}\")\n",
    "    input_size = shape[1]\n",
    "    compression = int(input_size * latent_dim / input_size)\n",
    "    reduction = compression // nlayers\n",
    "    if input_size - (nlayers-1)*reduction <= 0:\n",
    "        print(f\"to many layers {nlayers} for compression {compression}\")\n",
    "    \n",
    "    \n",
    "    # ------ encoder\n",
    "    inputer = None\n",
    "    encoder = None\n",
    "    if nlayers == 1:\n",
    "        inputer = Input(input_size)\n",
    "        #shap = (None \n",
    "        encoder = layers.Dense(input_size - (nlayers - 1)*reduction,\\\n",
    "                       activation=keras.activations.sigmoid)(inputer)\n",
    "        # we must have size bigger than 0 so check maximum for those number of layers\n",
    "    else:\n",
    "        # add linear layer\n",
    "        inputer = Input(input_size)\n",
    "        encoder = layers.Dense(input_size - reduction,\\\n",
    "                                   activation = layers.LeakyReLU(alpha = alfa))(inputer)\n",
    "        # add leaky relus\n",
    "        counter = 2\n",
    "        for i in range(nlayers - 3):\n",
    "            encoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                                   activation = layers.LeakyReLU(alpha = alfa))(encoder)\n",
    "            counter += 1\n",
    "        \n",
    "        # add last dense layer\n",
    "        encoder = layers.Dense(input_size - (nlayers - 1)*reduction,\\\n",
    "                               activation=keras.activations.sigmoid)(encoder)\n",
    " \n",
    "    \n",
    "    # --------- Latent log variance and mu layers    \n",
    "    fc_logvar = layers.Dense(latent_dim, name = \"log_var\")(encoder)\n",
    "    fc_mu = layers.Dense(latent_dim, name = \"mean\")(encoder)\n",
    "    fc = Sampling()([fc_mu, fc_logvar])\n",
    "    encod = keras.Model(inputer, [fc_mu, fc_logvar, fc], name=\"encoder\")\n",
    "    encod.summary()\n",
    "    \n",
    "    \n",
    "    # ------ decoder\n",
    "    inputer = keras.Input(latent_dim)\n",
    "    decoder = None\n",
    "    if nlayers == 1:\n",
    "        decoder = layers.Dense(input_size, activation = keras.activations.sigmoid)(inputer)\n",
    "        # we must have size bigger than 0 so check maximum for those number of layers\n",
    "    else:\n",
    "        counter = nlayers - 1\n",
    "        decoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                               activation = layers.LeakyReLU( alpha = alfa))(inputer)\n",
    "        \n",
    "        counter -= 1\n",
    "        for i in range(nlayers - 2):\n",
    "            decoder = layers.Dense(input_size - counter * reduction,\\\n",
    "                                   activation = layers.LeakyReLU( alpha = alfa))(decoder)\n",
    "            counter -= 1\n",
    "\n",
    "        # add last dense layer\n",
    "        decoder = layers.Dense(input_size, activation=keras.activations.sigmoid)(decoder)        \n",
    "    \n",
    "    decod = keras.Model(inputer, decoder, name=\"decoder\")\n",
    "    decod.summary()\n",
    "    \n",
    "    return encod,decod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86df192-bd99-473d-88bb-762d86fbec45",
   "metadata": {},
   "source": [
    "### Variational nn class keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633fa7c-54bb-48de-bcec-0dddc0d3e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder,epochs, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        # LOG AND MEAN ARE IN THE ENCODER ALREADY\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            weight = 0.85 * (self.epoch / self.epochs)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss * weight  #* self.weight\n",
    "        \n",
    "            #print(f\"with weight = {self.weight} and {total_loss}\")    \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    def scheduler(self, epoch, lr):\n",
    "\n",
    "        self.epoch = epoch\n",
    "\n",
    "        #print(self.weight)\n",
    "        return lr#self.weight\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2759c4-42c7-48fe-b408-aeb12c5b141c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AUTOENCODER PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66760d64-dbce-4bc5-b66f-896df911939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Print(nn.Module):\n",
    "    \"\"\"\n",
    "    Used to print the shape of the data inside of nn.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Print, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c9d46-ea1a-4274-b377-769d6ff2ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(input_size, n_layers, compression):\n",
    "    \n",
    "    reduction = compression // n_layers\n",
    "    encoder = None\n",
    "    # These if statements determine the structure of the VAE dependent on the desired compression\n",
    "    print(input_size, reduction)\n",
    "    if n_layers == 1:\n",
    "        # Desired structure for 1 total layer in the encoder\n",
    "        encoder = nn.Sequential(nn.Sigmoid())\n",
    "    elif n_layers == 2:\n",
    "        # Desired structure for 2 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 3:\n",
    "        # Desired structure for 3 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 4:\n",
    "        # Desired structure for 4 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 5:\n",
    "        # Desired structure for 5 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 6:\n",
    "        # Desired structure for 6 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 7:\n",
    "        # Desired structure for 7 total layers in the encoder\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size - reduction),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    decoder = None\n",
    "\n",
    "    if n_layers == 7:\n",
    "        # Desired structure for 7 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 6,\n",
    "                      input_size - reduction * 5),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif (n_layers == 6):\n",
    "        # Desired structure for 6 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 5,\n",
    "                      input_size - reduction * 4),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif (n_layers == 5):\n",
    "        # Desired structure for 5 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 4,\n",
    "                      input_size - reduction * 3),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 4:\n",
    "        # Desired structure for 4 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 3,\n",
    "                      input_size - reduction * 2),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 3:\n",
    "        # Desired structure for 3 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 2,\n",
    "                      input_size - reduction * 1),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 2:\n",
    "        # Desired structure for 2 total layers in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(compression, input_size - reduction * (n_layers - 1)),\n",
    "            nn.LeakyReLU(0.20),\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    elif n_layers == 1:\n",
    "        # Desired structure for 1 total layer in the decoder\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(input_size - reduction * 1, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Latent log variance and mu layers\n",
    "    fc_logvar = nn.Linear(input_size - reduction * (n_layers - 1), compression)\n",
    "    fc_mu = nn.Linear(input_size - reduction * (n_layers - 1), compression)\n",
    "\n",
    "\n",
    "\n",
    "    return {'decoder': decoder, 'encoder': encoder, 'logvar': fc_logvar, 'mu': fc_mu}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72c735-29c2-4065-9d3b-8ba5bcb7a1c3",
   "metadata": {},
   "source": [
    "### load file from folder like dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538dec57-bde8-434a-a487-863abdbc2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own class LoadFromFolder\n",
    "class FromFolder(Dataset):\n",
    "    def __init__(self, main_dir, filenum = None):\n",
    "         \n",
    "        # Set the loading directory\n",
    "        self.main_dir = main_dir\n",
    "        # List all images in folder and count them\n",
    "        all_files = list(filter(lambda x: x.endswith('.dat'), os.listdir(self.main_dir)))\n",
    "        if filenum != None:\n",
    "            all_files = all_files[0:filenum]\n",
    "        self.total_files = sorted(all_files)\n",
    "    def __len__(self):\n",
    "        # Return the previously computed number of images\n",
    "        return len(self.total_files)   \n",
    "    def __getitem__(self, idx):\n",
    "        loc = os.path.join(self.main_dir, self.total_files[idx])\n",
    "        fil = np.square(np.genfromtxt(loc))       \n",
    "        return fil\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54becccd-56da-422a-980a-2c5dee46d181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model class in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2272c-abd5-4d55-90b0-a23ef62fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelik:\n",
    "    def __init__(self, parameters, data_path, lat_dim, verbosity=0, n_layers=3, n_qubits=8,trainsize=0.9,filenum = None, load=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parameters: dict of json params\n",
    "            n_layers: number of layers in the encoder/decoder\n",
    "            n_qubits: number of lattice sites\n",
    "            load: optional path to load a pretrained model\n",
    "        \"\"\"\n",
    "        # Initialize class parameteres\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = n_qubits\n",
    "        self.N = int(math.pow(2, self.n_qubits))\n",
    "        self.lat_dim = lat_dim\n",
    "        self.compression = self.lat_dim / self.N\n",
    "        \n",
    "        self.epochs = int(parameters['epochs'])\n",
    "        self.batch_size = int(parameters['batch_size'])\n",
    "        self.trainsize = trainsize\n",
    "        self.display_epochs = int(parameters['display_epoch'])\n",
    "        self.learning_rate = parameters['learning_rate']\n",
    "        self.num_batches = int(parameters['num_batches'])\n",
    "        self.data_path = data_path\n",
    "        self.filenum = filenum\n",
    "\n",
    "\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        self.savename = self.data_path + f'..{kPSep}myModel_latent={self.lat_dim},{self.N}'\n",
    "\n",
    "        # Prepare model\n",
    "        self.vae, self.train_loaders, self.test_loaders, self.optimizer = self.prepare_model(\n",
    "            load=load)\n",
    "\n",
    "        # Train the model if it wasn't loaded, and compute fidelity\n",
    "        if load == None:\n",
    "            train_losses, test_losses = self.run_model()\n",
    "            self.plot_losses(train_losses, test_losses)\n",
    "        \n",
    "        \n",
    "        #print(self.train_loaders.dataset)\n",
    "        self.fidelity = self.get_fidelity(self.train_loaders)\n",
    "\n",
    "    def prepare_model(self, load=None):\n",
    "        \"\"\"\n",
    "        Initializes VAE model and loads it onto the appropriate device.\n",
    "        Reads and loads the data in the form of an array of Torch DataLoaders.\n",
    "        Initializes Adam optimizer.\n",
    "        Args:\n",
    "            load: path to load trained model from\n",
    "        Returns:\n",
    "            VAE\n",
    "            Array of train Torch Dataloaders\n",
    "            Array of test Torch Dataloaders\n",
    "            Adam optimizer\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        input_size = self.N\n",
    "        VAE_layers = get_layers(input_size, self.n_layers, self.lat_dim)\n",
    "        \n",
    "        vae = VariationalAutoencoder(VAE_layers.get('encoder'), VAE_layers.get(\n",
    "            'decoder'), VAE_layers.get('logvar'), VAE_layers.get('mu')).double().to(self.device)\n",
    "        \n",
    "        train_loaders, test_loaders = self.get_data(self.batch_size, self.data_path)\n",
    "    \n",
    "        optimizer = optim.Adam(vae.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        if not load == None:\n",
    "            vae.load_state_dict(torch.load(load))\n",
    "            vae.eval()\n",
    "\n",
    "        return vae, train_loaders, test_loaders, optimizer\n",
    "\n",
    "    def loss_function(self, x, x_reconstruction, mu, log_var, weight=1):\n",
    "        \"\"\"\n",
    "        Returns the loss for the model based on the reconstruction likelihood and KL divergence\n",
    "        Args:\n",
    "            x: Input data\n",
    "            x_reconstruction: Reconstructed data\n",
    "            mu:\n",
    "            log_var:\n",
    "            weight:\n",
    "        Returns:\n",
    "            loss:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        reconstruction_likelihood = F.binary_cross_entropy(\n",
    "            x_reconstruction, x, reduction='sum')\n",
    "        kl_divergence = -0.5 * \\\n",
    "                        torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        loss = reconstruction_likelihood + kl_divergence * weight\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_fidelity(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the reconstruction fidelity.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data\n",
    "        Returns:\n",
    "            out: Fidelity for the input sample\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        #self.vae.eval()\n",
    "        #torch.no_grad()\n",
    "        fidel = 0\n",
    "        counter = 0\n",
    "\n",
    "        x = x.dataset\n",
    "        #x = x.dot(1 << np.arange(x.shape[-1] - 1, -1, -1))  # Converts binary string to integer\n",
    "        print(x)\n",
    "        \n",
    "        l, u = x.min(), x.max() + 1\n",
    "        f1, b = np.histogram(x, density=True, bins=np.arange(l, u, 1))\n",
    "        \n",
    "        # Initialize for getting reconstructed density\n",
    "        f2 = np.zeros(f1.shape)\n",
    "        ns = 0\n",
    "        dim = int(self.n_qubits * self.compression)\n",
    "        while ns < 10:\n",
    "            # Get samples, decode them, convert to int, and add to hist count\n",
    "            re = np.random.multivariate_normal(\n",
    "                np.zeros(dim), np.eye(dim), size=int(0.375e7))\n",
    "            re = self.vae.decode(torch.Tensor(re).double().to(\n",
    "                self.device)).cpu().detach().numpy()\n",
    "            f2 += np.histogram(re, bins=b)[0]\n",
    "            ns += 1\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #    for i, data in enumerate(x):\n",
    "        #        if i >= self.num_batches:\n",
    "        #            break\n",
    "#\n",
    "        #        #print(data)\n",
    "        #        tmp = data.to(self.device)\n",
    "        #        reconstruction_data, mu, logvar = self.vae(tmp)\n",
    "        #        reconstruction_data = reconstruction_data.cpu()\n",
    "        #        #print(reconstruction_data)\n",
    "        #        for e in range(len(reconstruction_data)):\n",
    "        #            a = reconstruction_data[e].numpy()\n",
    "        #            b = data[e].numpy()\n",
    "        #            fidel += theirFidelity(a, b)\n",
    "        #            counter+=1\n",
    "        \n",
    "        db = np.array(np.diff(b), float)\n",
    "        f2 = f2 / db / f2.sum()\n",
    "\n",
    "        out = np.sum(np.sqrt(np.multiply(f1, f2)))\n",
    "        print(f\"Fidelity: {out}\")\n",
    "        del re, x_re, f1, f2, x\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return out #fidel/counter\n",
    "\n",
    "    def train(self, epoch, loader):\n",
    "        \"\"\"\n",
    "        Trains the VAE model\n",
    "        Args:\n",
    "            epoch: Number of current epoch to print\n",
    "            loader: Torch DataLoader for a quantum state\n",
    "        Returns:\n",
    "            epoch_loss: Loss for the epoch\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        self.vae.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, data in enumerate(loader):\n",
    "\n",
    "            if i >= self.num_batches:\n",
    "                break\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            reconstruction_data, mu, log_var = self.vae(data)\n",
    "            \n",
    "            loss = self.loss_function(\n",
    "                data, reconstruction_data, mu, log_var, weight=0.85 * (epoch / self.epochs))\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item() / (data.size(0) * self.num_batches)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (self.verbosity == 0 or (\n",
    "                    self.verbosity == 1 and (epoch + 1) % self.display_epochs == 0)) and i % self.batch_size == 0:\n",
    "                print(\"Done batch: \" + str(i) +\n",
    "                      \"\\tCurr Loss: \" + str(epoch_loss))\n",
    "\n",
    "        if self.verbosity == 0 or (self.verbosity == 1 and (epoch + 1) % self.display_epochs == 0):\n",
    "            print('Epoch [{}/{}]'.format(epoch + 1, self.epochs) +\n",
    "                  '\\tLoss: {:.4f}'.format(epoch_loss)\n",
    "                  )\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def test(self, epoch, loader):\n",
    "        \"\"\"\n",
    "        Tests VAE model\n",
    "        Args:\n",
    "            epoch: Number of current epoch to print\n",
    "            loader: Torch DataLoader for a quantum state\n",
    "        Returns:\n",
    "            epoch_loss: Loss for the epoch\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        self.vae.eval()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(loader):\n",
    "\n",
    "                if i >= self.num_batches:\n",
    "                    break\n",
    "\n",
    "                data = data.to(self.device)\n",
    "                reconstruction_data, mu, logvar = self.vae(data)\n",
    "                loss = self.loss_function(\n",
    "                    data, reconstruction_data, mu, logvar)\n",
    "                epoch_loss += loss.item() / (data.size(0) * self.num_batches)\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def run_model(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: Quantum state the model will be trained on\n",
    "        Returns:\n",
    "            test and training loss\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader, test_loader = self.train_loaders, self.test_loaders\n",
    "        train_losses, test_losses = [], []\n",
    "\n",
    "        print(\"Beginning Training:\")\n",
    "        for e in range(0, self.epochs):\n",
    "            train_loss = self.train(e, train_loader)\n",
    "            test_loss = self.test(e, test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Final train loss: {train_loss}\\tFinal test loss: {test_loss}\")\n",
    "\n",
    "        #torch.save(self.vae.state_dict(),self.savename)\n",
    "\n",
    "        return train_losses, test_losses\n",
    "\n",
    "    def plot_losses(self, train_losses, test_losses):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_losses: list of training losses from run_model\n",
    "            test_losses: list of testing losses from run_model\n",
    "        Returns:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        savename=self.savename\n",
    "        epochs = np.arange(0, len(train_losses), 1)\n",
    "        plt.plot(epochs, train_losses, \"g-\", label=\"Training Loss\")\n",
    "        plt.plot(epochs, test_losses, \"b-\", label=\"Testing Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"VAE Training Loss with {self.n_layers} layers\")\n",
    "        plt.legend()\n",
    "        plt.xlim(0, len(train_losses))\n",
    "        \n",
    "        figure_num = 1\n",
    "        while os.path.exists(f'{savename}_loss_{figure_num}.png'):\n",
    "            figure_num += 1\n",
    "        plt.savefig(f'{savename}_loss_{figure_num}.png')\n",
    "        plt.clf()\n",
    "        print(f'{savename}_loss_{figure_num}.png')\n",
    "\n",
    "    def plot_fidelities(self, fs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fs: A list of Fidelities from each model\n",
    "        Returns:\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        \n",
    "        savename=self.savename\n",
    "        epochs = np.arange(1, len(fs) + 1, 1)\n",
    "        plt.plot(epochs, fs, \"b--o\", label=\"Fidelity\")\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.xticks(ticks=epochs)\n",
    "        plt.ylabel(\"Fidelity\")\n",
    "        plt.title(\"VAE Fidelities\")\n",
    "        plt.xlim(epochs.min(), epochs.max())\n",
    "        \n",
    "        \n",
    "        figure_num = 1\n",
    "        while os.path.exists(f'{savename}_fidelity_{figure_num}.png'):\n",
    "            figure_num += 1\n",
    "        plt.savefig(f'{savename}_fidelity_{figure_num}.png')\n",
    "        plt.clf()\n",
    "        print(f'{savename}_fidelity_{figure_num}.png')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_data(self, batch_size, file_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: Size of batches\n",
    "            file_path: Path of file location\n",
    "        Returns:\n",
    "            train_loaders: Array of Torch DataLoaders representing quantum states for training\n",
    "            test_loaders: Array of Torch DataLoaders representing quantum states for testing\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        ds = FromFolder(file_path, self.filenum)\n",
    "        num = ds.__len__()\n",
    "        num_train = int(num * self.trainsize)\n",
    "        train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(ds, (num_train, num - num_train, 0))\n",
    "        \n",
    "        train_loaders  = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loaders  = torch.utils.data.DataLoader(\n",
    "            valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc43c8-49d3-4b92-ac03-20f64f649386",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variational nn class pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ceb59c-7c21-4557-888b-e900b1ffccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder class.\n",
    "    Architecture:\n",
    "        - x Fully connected layers\n",
    "        - Sigmoid activation function\n",
    "        - LeakyReLU activation function with slope of -0.2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encode, decode, logvar, mu):\n",
    "        \"\"\"\n",
    "        Very standard VAE, all the heavy lifting done elsewhere\n",
    "        Args:\n",
    "            encode: encoder input from hidden_layers\n",
    "            decode: decoder layers\n",
    "            logvar: logvar layer\n",
    "            mu:mu layer\n",
    "        \"\"\"\n",
    "\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.LReLU = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fc_logvar = logvar\n",
    "        self.fc_mu = mu\n",
    "\n",
    "        self.encode = encode\n",
    "        self.decode = decode\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.encode(x)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return reconstruction, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ce0a4-1e6c-40a2-8cf1-8d147acde623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b2c8dd8-f351-4f38-bc50-561a482f5054",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942a082-fe85-430d-9b2a-1c95b8dfe4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileAutoencode(directory, model, latent_dim, epo,layer_num, trainsize = 0.8,filenum = None, batch = 10, verbose = 2,\\\n",
    "                   activation = 'relu', savename = 'model', save = True, savefiles = False):\n",
    "    folderLog = directory + \"_\" + model.getInfo() + kPSep\n",
    "    folder = folderLog + \"wavefunctions\" + kPSep\n",
    "    folderSaveNew = folderLog + \"wavefunctions_encoder\" + kPSep\n",
    "    createFolder([folderSaveNew])\n",
    "    savename = folderLog + f'myModel_latent={latent_dim/model.N:.3f}'\n",
    "    # read files\n",
    "    wavefuns = []\n",
    "    \n",
    "    files = list(filter(lambda x: x.endswith('.dat'), os.listdir(folder)))\n",
    "    maximum = len(files)\n",
    "    if filenum != None:\n",
    "        maximum = filenum\n",
    "    \n",
    "\n",
    "    counter = 0\n",
    "    # create squares of the wavefunctions\n",
    "    for filename in files:\n",
    "        tmp = np.square(np.genfromtxt(folder+filename))\n",
    "        wavefuns.append(tmp)\n",
    "        if counter == maximum:\n",
    "            break\n",
    "        counter+=1\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(wavefuns, wavefuns, test_size=1-trainsize, shuffle= True)\n",
    "    tmp = np.concatenate([x_train, y_train])\n",
    "    \n",
    "    #print(x_train, y_train)\n",
    "    # separate training etc\n",
    "    #data_train = np.array(wavefuns[0:int(trainsize*maximum)])\n",
    "    #data_test = np.array(wavefuns[int(trainsize*maximum):])\n",
    "    #print(data_test)\n",
    "    \n",
    "    print(f'\\n\\n\\t\\tMaking autoencoder with latent={latent_dim/model.N}\\t\\t\\n\\n')    \n",
    "    encoder, decoder = autoEnc(latent_dim, (None, int(modelik.N)),layer_num, alfa = 0.2)\n",
    "    vae = VAE(encoder, decoder, epo)\n",
    "    # first model compile\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(lr=1e-3))\n",
    "    vae.build((None, int(modelik.N)))\n",
    "    \n",
    "    #story = vae.fit(tmp,\n",
    "    #epochs=1,\n",
    "   #shuffle=False,\n",
    "    #batch_size = batch,\n",
    "    #verbose=0)\n",
    "    \n",
    "    vae.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # making callbacks\n",
    "    early_stopping_cb = callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "    callback = [early_stopping_cb]\n",
    "    if save:\n",
    "        callback.append(keras.callbacks.ModelCheckpoint(savename + \".h5\", save_best_only=True))\n",
    "    if verbose == 2:\n",
    "        callback.append(TqdmCallback(verbose = verbose))\n",
    "    \n",
    "    # define number of iterations in training and test\n",
    "    train_iter = round(np.array(x_train).shape[0]/batch)\n",
    "    test_iter = round(np.array(x_valid).shape[0]/batch)\n",
    "    \n",
    "    # make artificial history\n",
    "    history = {}\n",
    "    history['history'] = {}\n",
    "    history['history']['total_loss'] = []\n",
    "    history['history']['reconstruction_loss'] = []\n",
    "    history['history']['kl_loss'] = []   \n",
    "    \n",
    "    tot_loss, reconstr_loss, kl_loss = 0, 0, 0\n",
    "    for ep in range(epo):\n",
    "        vae.epoch = ep\n",
    "        vae.compile(optimizer=keras.optimizers.Adam(lr=1e-3))\n",
    "        # train iterations\n",
    "        for i in range(train_iter):\n",
    "            start = i*batch\n",
    "            end = i*batch + batch\n",
    "            batchX = np.array(x_train[start:end]).astype('float32')\n",
    "            \n",
    "            #batchY = np.array(y_train[start:end]).astype('float32')\n",
    "            #print(batchX.shape)\n",
    "            #tmp = np.concatenate([batchX, batchY])\n",
    "            #tmp = np.expand_dims(tmp, -1).astype(\"float32\")\n",
    "            print(batchX, batchX.shape)\n",
    "\n",
    "           #story = vae.fit(tmp,\n",
    "           #  epochs=1,\n",
    "           #  shuffle=False,\n",
    "           #  batch_size = batch,\n",
    "           #  verbose=0,\n",
    "           #  callbacks = callback)\n",
    "            tot_loss_,reconstr_loss_,kl_loss_ = vae.train_on_batch(batchX)\n",
    "            #print(dic)\n",
    "            #tot_loss_ = dic['loss']\n",
    "            #reconstr_loss_ = dic['reconstruction_loss'] \n",
    "            #kl_loss_ = dic['kl_loss']\n",
    "\n",
    "            #print(tot_loss_, reconstr_loss_, kl_loss_)\n",
    "            tot_loss += tot_loss_\n",
    "            reconstr_loss += reconstr_loss_\n",
    "            kl_loss += kl_loss_\n",
    "        history['history']['total_loss'].append(tot_loss/train_iter)\n",
    "        history['history']['reconstruction_loss'].append(reconstr_loss/train_iter)\n",
    "        history['history']['kl_loss'].append(kl_loss/train_iter)    \n",
    "        \n",
    "        # test iterations \n",
    "        #val_tot_loss, val_reconstr_loss, val_kl_loss = 0, 0, 0\n",
    "        #for i in range(test_iter):\n",
    "        #    start = i*batch\n",
    "        #    end = i*batch + batch\n",
    "        #    batchX = np.array(x_valid[start:end]).astype('float32')\n",
    "        #    batchY = np.array(y_valid[start:end]).astype('float32')\n",
    "        #    \n",
    "        #    tmp = np.concatenate([batchX, batchY], axis = 0)\n",
    "        #    #tmp = np.expand_dims(tmp, -1).astype(\"float32\")\n",
    "        #    #istory = vae.fit(tmp,\n",
    "        #    #   epochs=1,\n",
    "        #    #   shuffle=False,\n",
    "        #    #   batch_size = batch,\n",
    "        #    #   verbose=0,\n",
    "        #    #   callbacks = callback)\n",
    "##\n",
    "        #    dic =vae.test_step([tmp])\n",
    "        #    val_tot_loss_ = dic['val_loss']\n",
    "        #    val_reconstr_loss_ = dic['val_reconstruction_loss'] \n",
    "        #    val_kl_loss_ = dic['val_kl_loss']\n",
    "    #\n",
    "    #\n",
    "        #    val_tot_loss += val_tot_loss_\n",
    "        #    val_reconstr_loss += val_reconstr_loss_\n",
    "        #    val_kl_loss += val_kl_loss_\n",
    "        #history['history']['val_total_loss'].append(val_tot_loss/test_iter)\n",
    "        #history['history']['val_reconstruction_loss'].append(val_reconstr_loss/test_iter)\n",
    "        #history['history']['val_kl_loss'].append(val_kl_loss/test_iter)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #histories.append(history)\n",
    "    \n",
    "    \n",
    "    # save file h5\n",
    "    if save:\n",
    "        print(\"\\t\\t---->creating h5 file: \" + savename + \".h5\")\n",
    "        vae.save(savename + \".h5\", save_format='h5')\n",
    "    \n",
    "            #print(history.history)\n",
    "    #validation_data=(data_test,data_test)\n",
    "    # plot model\n",
    "    print(history['history']['total_loss'])\n",
    "    \n",
    "    #plot_history(history, path=savename+',training.png')\n",
    "    \n",
    "    counter = 0\n",
    "    fidelity = 0\n",
    "    # save new files\n",
    "    if savefiles:\n",
    "        for file in wavefuns:\n",
    "            name = f\"{counter}_wavefun__{model.getInfo()}.txt\"\n",
    "            # predict probabilities\n",
    "            tmp = np.array(vae.decoder.predict(np.array([file]))[0])\n",
    "            #print(tmp-file)\n",
    "\n",
    "            # save to file\n",
    "            fil = open(folderSaveNew + name, \"wb\")\n",
    "            np.save(fil, tmp)\n",
    "            fil.close()\n",
    "\n",
    "            fidelity += theirFidelity(tmp, file)\n",
    "            if counter == maximum:\n",
    "                break\n",
    "            counter+=1\n",
    "    else:\n",
    "        for i in np.random.randint(len(wavefuns), size=batch):\n",
    "            file = wavefuns[i]\n",
    "            \n",
    "            # predict probabilities\n",
    "            tmp = np.array(vae.encoder.predict(np.array([file]))[0])\n",
    "            tmp = np.array(vae.decoder.predict([tmp])[0])\n",
    "            #print(file, tmp, \"\\n\\n\\n\")\n",
    "            \n",
    "            # fidelity\n",
    "            fidelity += theirFidelity(tmp, file)\n",
    "            if counter == maximum:\n",
    "                break\n",
    "            counter+=1\n",
    "            \n",
    "    return vae, fidelity/counter#,entropy_before/counter, entropy/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a42081-f2f4-4277-9465-32ba41c377f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518fc553-4649-464c-bfc7-765ac58c0fbd",
   "metadata": {},
   "source": [
    "# LOOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d787c-b4fc-4e33-a1af-b466ca12d092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3fcb1-d200-4cef-bc8d-c00129135863",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = f\"{IsingPath}{kPSep}resultsWAVE5{kPSep}disorder{kPSep}PBC{kPSep}\"\n",
    "modelik = IsingDisorder(12,1,0,0.8,0,0.83,0.1,0)\n",
    "N=modelik.N\n",
    "L = 12\n",
    "latent_dims = np.linspace(5, N, 30)\n",
    "ws = np.array([0.1*i for i in range(1,2)])\n",
    "epo = 10\n",
    "batch = 10\n",
    "filenum = 10\n",
    "layer_num = 2\n",
    "# for pytorch mode\n",
    "parameters = {\"epochs\" : epo, 'batch_size' : batch, 'display_epoch' : 5, 'learning_rate' : 1e-3, 'num_batches':batch}\n",
    "\n",
    "ws, latent_dims, destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b677303-7d99-4317-b0f6-e4cd572d2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43931a1-6cd2-4a2b-9117-4f2c4ab5904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15e199-6eea-431b-8c62-76314d03fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb808a0-0084-4855-a0f5-ab2ea42d606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer_num in range(3,4):\n",
    "    with open(destination + f\"heatmap_layernum={layer_num},filenum={filenum},L={L},epo={epo},batch={batch}.txt\", \"w\") as mapka:\n",
    "        for w in ws:\n",
    "            modelik = IsingDisorder(L,1,0,0.8,0,0.83,w,0)\n",
    "            N=modelik.N\n",
    "            folderLog = destination + \"_\" + modelik.getInfo() + kPSep\n",
    "            folder = folderLog + \"wavefunctions\" + kPSep\n",
    "            \n",
    "            for lat_dim in latent_dims:\n",
    "                modelik2 = Modelik(parameters, folder, lat_dim = int(lat_dim), verbosity=0,\n",
    "                                   n_layers=layer_num,\n",
    "                                   n_qubits=L,filenum = filenum,\n",
    "                                   load=None)\n",
    "                #enc,fidelity = fileAutoencode(destination, modelik, int(lat_dim), epo,\n",
    "                #                              layer_num=layer_num,\n",
    "                #                              filenum=filenum,batch=batch,\n",
    "                #                              verbose = 0,\n",
    "                #                              save = False, savefiles = False)\n",
    "                fid = modelik2.fidelity\n",
    "                justPrinter(mapka, \"\\t\", [f\"{w:.2f}\", f\"{int(lat_dim) / float(N):.5f}\"\n",
    "                                          ,f\"{fid:.7f}\"], width = 10)\n",
    "                mapka.flush()\n",
    "\n",
    "#kullback_leibler_divergence\n",
    "#mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b88c21-5c27-4fc6-a8ce-a855d81768c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2251a-5b71-4c10-80ab-035db2bcdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims([[1,2],[3,4]], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dc623-ee3c-45c3-a792-8106030a50be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aaf48-5f29-4659-91eb-8957f0b07fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b15a38-4f9b-4018-9e11-768e279316dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b40a3-23fb-4e24-ab3e-dea5fea9cc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89e925-c935-4afb-a154-785c5ae72eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
